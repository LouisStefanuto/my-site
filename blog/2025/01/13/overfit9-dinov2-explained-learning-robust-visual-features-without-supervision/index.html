
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Personal website of Louis Stefanuto">
      
      
      
      
        <link rel="prev" href="../../../../2024/05/19/overfit8-alphafold2-the-structure-network/">
      
      
        <link rel="next" href="../../../02/01/overfit10-deepseek-r1/">
      
      
      <link rel="icon" href="../../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.18">
    
    
      
        <title>Overfit#9: DINOv2 explained: Learning robust visual features without supervision - Louis Stefanuto</title>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/stylesheets/main.7e37652d.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#overfit9-dinov2-explained-learning-robust-visual-features-without-supervision" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../../.." title="Louis Stefanuto" class="md-header__button md-logo" aria-label="Louis Stefanuto" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Louis Stefanuto
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Overfit#9: DINOv2 explained: Learning robust visual features without supervision
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
    
      <div class="md-header__source">
        <a href="https://github.com/LouisStefanuto" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    Github
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../about/" class="md-tabs__link">
        
  
  
    
  
  About

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../../../" class="md-tabs__link">
          
  
  
  Posts

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../../.." title="Louis Stefanuto" class="md-nav__button md-logo" aria-label="Louis Stefanuto" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Louis Stefanuto
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/LouisStefanuto" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    Github
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../about/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    About
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Posts
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Posts
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      <a href="../../../../" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Index
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Post series
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            Post series
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/alphafold/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AlphaFold
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/chemistry/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Chemistry
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/computer-vision/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Computer Vision
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/diffusion-models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Diffusion models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/drug-discovery/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Drug Discovery
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/embedding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Embedding
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/llm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LLM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/reinforcement-learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Reinforcement Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/robotics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Robotics
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/ssl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SSL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/video/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Video
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#motivation" class="md-nav__link">
    <span class="md-ellipsis">
      Motivation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#a-gentle-introduction-to-contrastive-learning" class="md-nav__link">
    <span class="md-ellipsis">
      A gentle introduction to Contrastive Learning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pre-training" class="md-nav__link">
    <span class="md-ellipsis">
      Pre-training
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Pre-training">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#image-level-loss-dino" class="md-nav__link">
    <span class="md-ellipsis">
      Image-level loss (DINO)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#patch-level-loss-ibot" class="md-nav__link">
    <span class="md-ellipsis">
      Patch-level loss (iBOT)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#koleo-regularizer" class="md-nav__link">
    <span class="md-ellipsis">
      KoLeo regularizer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training-stabilization" class="md-nav__link">
    <span class="md-ellipsis">
      Training stabilization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#swav-centering" class="md-nav__link">
    <span class="md-ellipsis">
      SwAV centering
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#high-resolution-training-phase" class="md-nav__link">
    <span class="md-ellipsis">
      High-resolution training phase
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#implementation-details" class="md-nav__link">
    <span class="md-ellipsis">
      Implementation details
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Implementation details">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dataset-curation" class="md-nav__link">
    <span class="md-ellipsis">
      Dataset curation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training-details" class="md-nav__link">
    <span class="md-ellipsis">
      Training details
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#results" class="md-nav__link">
    <span class="md-ellipsis">
      Results
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#limitations-and-extensions" class="md-nav__link">
    <span class="md-ellipsis">
      Limitations and extensions
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      References
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
  <div class="md-content md-content--post" data-md-component="content">
    <div class="md-sidebar md-sidebar--post" data-md-component="sidebar" data-md-type="navigation">
      <div class="md-sidebar__scrollwrap">
        <div class="md-sidebar__inner md-post">
          <nav class="md-nav md-nav--primary">
            <div class="md-post__back">
              <div class="md-nav__title md-nav__container">
                <a href="../../../../" class="md-nav__link">
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
                  <span class="md-ellipsis">
                    Back to index
                  </span>
                </a>
              </div>
            </div>
            
              <div class="md-post__authors md-typeset">
                
                  <div class="md-profile md-post__profile">
                    <span class="md-author md-author--long">
                      <img src="https://github.com/LouisStefanuto.png" alt="Louis Stefanuto">
                    </span>
                    <span class="md-profile__description">
                      <strong>
                        
                          Louis Stefanuto
                        
                      </strong>
                      <br>
                      Creator
                    </span>
                  </div>
                
              </div>
            
            <ul class="md-post__meta md-nav__list">
              <li class="md-nav__item md-nav__item--section">
                <div class="md-post__title">
                  <span class="md-ellipsis">
                    Metadata
                  </span>
                </div>
                <nav class="md-nav">
                  <ul class="md-nav__list">
                    <li class="md-nav__item">
                      <div class="md-nav__link">
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 19H5V8h14m-3-7v2H8V1H6v2H5c-1.11 0-2 .89-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2h-1V1m-1 11h-5v5h5z"/></svg>
                        <time datetime="2025-01-13 00:00:00+00:00" class="md-ellipsis">January 13, 2025</time>
                      </div>
                    </li>
                    
                    
                      <li class="md-nav__item">
                        <div class="md-nav__link">
                          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9 3v15h3V3zm3 2 4 13 3-1-4-13zM5 5v13h3V5zM3 19v2h18v-2z"/></svg>
                          <span class="md-ellipsis">
                            in
                            
                              <a href="../../../../category/ssl/">SSL</a>, 
                              <a href="../../../../category/computer-vision/">Computer Vision</a>, 
                              <a href="../../../../category/embedding/">Embedding</a></span>
                        </div>
                      </li>
                    
                    
                      
                      <li class="md-nav__item">
                        <div class="md-nav__link">
                          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 20a8 8 0 0 0 8-8 8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8m0-18a10 10 0 0 1 10 10 10 10 0 0 1-10 10C6.47 22 2 17.5 2 12A10 10 0 0 1 12 2m.5 5v5.25l4.5 2.67-.75 1.23L11 13V7z"/></svg>
                          <span class="md-ellipsis">
                            
                              18 min read
                            
                          </span>
                        </div>
                      </li>
                    
                  </ul>
                </nav>
              </li>
            </ul>
            
          </nav>
          
        </div>
      </div>
    </div>
    <article class="md-content__inner md-typeset">
      
        
  



  
  


<h1 id="overfit9-dinov2-explained-learning-robust-visual-features-without-supervision"><strong>Overfit#9:</strong> DINOv2 explained: Learning robust visual features without supervision</h1>
<p><a href="./"><img alt="main dinov2" src="../../../../images/9/main.jpg" /></a></p>
<!-- more -->

<h2 id="motivation">Motivation</h2>
<p>Every Data Scientist once used a ResNet50 backbone pre-trained on ImageNet to <strong>extract visual features</strong> for classification or segmentation. These models are usually pre-trained by keeping the encoder part of a CNN after a supervised classification training.</p>
<blockquote>
<p>Don't get me wrong. Such models perform well for most use cases. BUT ...</p>
</blockquote>
<p>The main weakness of these models is their training dataset. First, Supervised Learning requires high-quality annotated datasets. Sadly for us, these datasets are expensive to scale. Second, these datasets often lack diversity and are not representative of real-life. This leads to poor features for out-of-domain (OOD) images.</p>
<p>Hopefully, this challenge is similar to the one faced with embedding models a few years ago in NLP. Researchers showed that we could leverage the knowledge of huge datasets with no labels, thus bypassing the need for annotation. They also discovered that the Transformers architecture performance scaled gracefully with the dataset and model sizes.</p>
<blockquote>
<p>Can we leverage these techniques to train a BERT-like embedding model for images?</p>
</blockquote>
<p><strong>DINOv2</strong><sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup> is a Self-Supervised Learning method to train image encoders, without supervision. It builds on top of previous SSL works for Computer Vision like ViT, Masked Auto Encoders and iBOT and achieves impressive results on pretty much every downstream task. It is the continuation of DINO<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup>, a previous work from Meta (2021) that paved the way toward self-<strong>DI</strong>stillation with <strong>NO</strong> labels.</p>
<p>Introduced by Meta in early 2023, DINOv2 has since settled itself as one of the <strong>go-to technique to train large visual encoders</strong>.</p>
<figure>
<p><img alt="gif" src="https://cdn.prod.website-files.com/60d1a7f6aeb33c5c595468b4/655e8059ed48672faddad0c0_dino1.gif" /></p>
<figcaption>Despite training without supervision, DINOv2's embeddings allow out-of-the-box segmentation with a simple PCA.</figcaption>
</figure>
<hr />
<h2 id="a-gentle-introduction-to-contrastive-learning">A gentle introduction to Contrastive Learning</h2>
<p>What is a good embedding model? A simple definition could be:</p>
<blockquote>
<p>Similar images should be close in the latent space. On the opposite, dissimilar images should to be far away from each other.</p>
</blockquote>
<p><strong>Contrastive learning</strong> is a set of methods to achieve this property. Historically, most contrastive methods rely on a positive pairs VS negative pairs paradigm. Positive pairs are similar images, i.e 2 pictures of dogs, of castles, of the same person. Negative pairs are pictures of dissimilar labels.</p>
<p>Building negative pairs is easy: sample two random images. If your dataset is large enough, drawing a similar image is highly unlikely. Building positive pairs is more complex because we have no annotation in our dataset. A simple technique is to augment an image (rotation, blur, contrast ...) to get two slightly different images.</p>
<blockquote>
<p>You now have the basics of contrastive learning. Let's review a standard contrastive learning technique - SimCLR - to understand its drawbacks. This will give us the motivation behind some unintuitive choices in DINOv2 and related work.</p>
</blockquote>
<p><strong>SimCLR</strong><sup id="fnref:7"><a class="footnote-ref" href="#fn:7">7</a></sup> (Simple framework for Contrastive Learning of visual Representations) puts these ideas into practice. Take a mini-batch of N images. Augment each image twice to build <strong>N positive</strong> pairs and <strong>2(N-1) negative</strong> pairs. Then train an embedding network to minimize the distance between positive pairs and to maximize the distance between negative pairs.</p>
<div class="admonition quote">
<p><figure markdown>
<img alt="simCLR" src="../../../../images/9/simclr.png" width="350" />
<figcaption>SimCLR.</figcaption>
</figure></p>
</div>
<p>SimCLR works well under one condition: it requires <strong>large batch sizes</strong> (&gt;1024). The plot below, shows the degradation of the performance with the batch size. Intuitively, that means that SimCLR requires a lot of negative samples to space the embeddings in the latent space.</p>
<p>This is problematic, because large batch sizes mean large memory usage. That is why DINO authors chose another technique called self-distillation, more on that later.</p>
<div class="admonition quote">
<p><figure markdown>
<img alt="simCLR" src="../../../../images/9/simclr_batchsize.png" width="350" />
<figcaption>The batch size has a huge impact on SimCLR performance.</figcaption>
</figure></p>
</div>
<h2 id="pre-training">Pre-training</h2>
<p>In the paper, we can read:</p>
<blockquote>
<p>We learn our features with a <strong>discriminative</strong> self-supervised method that can be seen as a combination of <strong>DINO</strong> and <strong>iBOT</strong> losses with the <strong>centering of SwAV</strong> (Caron et al., 2020). We also add a <strong>[Koleo] regularizer</strong> to spread features and a short high-resolution training phase.</p>
</blockquote>
<p>To keep things simple, let's ignore SwAV for the moment. We will come back to it later. What the authors are telling us is that DINOv2 loss is a sum of three terms. Let's dive into each term to understand the motivation behind it.</p>
<div class="admonition bug">
<div class="arithmatex">\[
\mathcal{Loss} = \mathcal{L}_{DINO} + \alpha \mathcal{L}_{iBOT} + \beta \mathcal{L}_{koleo}
\]</div>
</div>
<h3 id="image-level-loss-dino">Image-level loss (DINO)</h3>
<p>In Transformers and ViT, the embedding of the whole sequence is often computed from an extra [CLS] token that summarizes the semantics of the sequence. For instance in classification tasks, the [CLS] embedding is fed to the MLP classifier to predict a class/a next token ...</p>
<p>The <strong>DINO loss</strong> aims at minimizing (resp. maximizing) the distance between the image embeddings of similar (resp. dissimilar) images. It is an image-level loss.</p>
<blockquote>
<p>How are these meaningful [CLS] embeddings learnt?</p>
</blockquote>
<p>DINO uses a different paradigm called <strong>self-distillation</strong>. Self-distillation is a process of training a student model <span class="arithmatex">\(\theta_s\)</span> to mimic the outputs of a teacher model <span class="arithmatex">\(\theta_t\)</span>.</p>
<p>DINO reuses the data augmentation idea, to generate <span class="arithmatex">\(V\)</span> versions of an image <span class="arithmatex">\(\mathbf{x}\)</span>, called <strong>views</strong>.The authors discriminate global views (large crops) and local views (small crops). Based on a local view <span class="arithmatex">\(\mathbf{x}_1\)</span>, the student network is then asked to predict the embeddings of the teacher network, that was fed with a more global view image <span class="arithmatex">\(\mathbf{x}_2\)</span>.</p>
<div class="admonition quote">
<p><figure markdown>
<img alt="gif" src="../../../../images/9/dino.png" width="400" />
<figcaption>DINO student-teacher self-distillation architecture.</figcaption>
</figure></p>
</div>
<blockquote>
<p>Why does that work?</p>
</blockquote>
<p>According to the authors this training technique encourages “local-to-global correspondences".</p>
<ol>
<li>The self-distillation process trains the network to embed the image regardless of the applied transformation = <strong>learns robustness</strong></li>
<li>In some sense, the student is asked to predict global features from a local crop of the image. This is similar to Masked Auto Encoders<sup id="fnref:4"><a class="footnote-ref" href="#fn:4">4</a></sup>, where the student reconstructs the patch from a condensed representation and learns to fill the cropped parts. Those tasks require a strong semantic understanding of the image by the student = <strong>learn image understanding</strong></li>
</ol>
<blockquote>
<p>What is the DINO loss?</p>
</blockquote>
<p>The student learns to mimic the output distribution of the teacher, a K-dimensional 1d-vector.</p>
<p><strong>Cross-entropy</strong> is thus a suitable loss function for this task. The teacher predictions <span class="arithmatex">\(p_t\)</span> are the targets. The student predictions <span class="arithmatex">\(p_s\)</span> are the predictions. CE requires scores between 0 and 1, so the student and teacher outputs go through a softmax activation function.</p>
<p>Note that the teacher outputs are centered and that the softmax contain a temperature parameter. More on these two in the <a href="#training-stabilization">Training stabilization</a> and <a href="#swav-centering">SwAV</a> sections.</p>
<div class="arithmatex">\[
\mathcal{L}_{DINO} = - \sum p_t \log p_s
\]</div>
<p>The teacher is built from a Exponential Moving Average (EMA) of the student. During the back-propagation, the teacher is frozen (stop-gradient), only the student is updated by gradients. The authors tried many other techniques like copying the last version of the student, but the performance decreased significantly or even collapsed.</p>
<p>To summarize, here is the pseudo-code for DINO<strong>v1</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># PSEUDO-CODE OF DINOv1, from Caron &amp; al. (2021)</span>

<span class="c1"># gs, gt: student and teacher networks</span>
<span class="c1"># C: center (K)</span>
<span class="c1"># tps, tpt: student and teacher temperatures</span>
<span class="c1"># l, m: network and center momentum rates</span>

<span class="n">gt</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">gs</span><span class="o">.</span><span class="n">params</span>
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>  <span class="c1"># load a minibatch x with n samples</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">augment</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">augment</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># random views</span>

    <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span> <span class="o">=</span> <span class="n">gs</span><span class="p">(</span><span class="n">x1</span><span class="p">),</span> <span class="n">gs</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>  <span class="c1"># student output n-by-K</span>
    <span class="n">t1</span><span class="p">,</span> <span class="n">t2</span> <span class="o">=</span> <span class="n">gt</span><span class="p">(</span><span class="n">x1</span><span class="p">),</span> <span class="n">gt</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>  <span class="c1"># teacher output n-by-K</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="n">H</span><span class="p">(</span><span class="n">t1</span><span class="p">,</span> <span class="n">s2</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">H</span><span class="p">(</span><span class="n">t2</span><span class="p">,</span> <span class="n">s1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># back-propagate</span>

    <span class="c1"># student, teacher and center updates</span>
    <span class="n">update</span><span class="p">(</span><span class="n">gs</span><span class="p">)</span>  <span class="c1"># SGD</span>
    <span class="n">gt</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">l</span> <span class="o">*</span> <span class="n">gt</span><span class="o">.</span><span class="n">params</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l</span><span class="p">)</span> <span class="o">*</span> <span class="n">gs</span><span class="o">.</span><span class="n">params</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">m</span> <span class="o">*</span> <span class="n">C</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">cat</span><span class="p">([</span><span class="n">t1</span><span class="p">,</span> <span class="n">t2</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">H</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>  <span class="c1"># stop gradient</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">s</span> <span class="o">/</span> <span class="n">tps</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">((</span><span class="n">t</span> <span class="o">-</span> <span class="n">C</span><span class="p">)</span> <span class="o">/</span> <span class="n">tpt</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># center + sharpen</span>
    <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">t</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="n">s</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div>
<blockquote>
<p>How different is the DINO's approach from SimCLR's?</p>
</blockquote>
<p>An interesting property of DINO is the minimal impact of small batch sizes. DINO supports much smaller batch sizes, like 128 enabling 1-GPU training for a 1B model!</p>
<p>Another major difference with SimCLR is the absence of an explicit repulsive/contrastive term. In DINOv1, the contrastive mechanism is implicit via diverse views. As we will see later, in DINOv2, the authors introduced additional implicit and explicit mechanisms to reinforce this effect.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>I intentionally let the SwAV centering under silence for the moment, to split concepts. DINOv1 didn't use it, and I think it is easier to explain its addition in a separate section. Details are coming, I promise.</p>
</div>
<h3 id="patch-level-loss-ibot">Patch-level loss (iBOT)</h3>
<p>The DINO loss ensures that the image-level embeddings are meaningful, but what about the patch embeddings? In an ideal setup, it would be nice that similar (resp. dissimilar) parts of an image have similar (resp. dissimilar) embeddings. For instance, meaningful and diverse patch embeddings would enable out-of-the-box segmentation of an image. Or finding the most similar patches in two images ... This is the intuition behind the <strong>iBOT</strong> loss (Image BERT pre-training with online tokenizer<sup id="fnref:5"><a class="footnote-ref" href="#fn:5">5</a></sup>).</p>
<div class="admonition quote">
<p><figure markdown>
<img alt="ibot" src="../../../../images/9/ibot.png" width="500" />
<figcaption>iBOT uses two views and then predicts the masked tokens of the other view.</figcaption>
</figure></p>
</div>
<blockquote>
<p><strong>Paper:</strong> We randomly mask some of the input patches given to the student, but not to the teacher. We then apply the student iBOT head to the student mask tokens. Similarly, we apply the teacher iBOT head to the (visible) teacher patch tokens corresponding to the ones masked in the student. We then apply the softmax and centering steps as above, and obtain the iBOT loss term.</p>
</blockquote>
<p>The task is really close to the BERT pre-training paradigm in NLP, in which the model predicts the masked tokens. Btw, the loss is almost the same as <span class="arithmatex">\(\mathcal{L}_{DINO}\)</span>:</p>
<div class="arithmatex">\[
\mathcal{L}_{iBOT} = - \sum_i p_{ti} \log p_{si}
\]</div>
<p>with <span class="arithmatex">\(i\)</span> the indices of the masked tokens.</p>
<blockquote>
<p><strong>Interesting fact</strong>: In the iBOT paper, the DINO and the iBOT heads were tied. In DINOv2, the authors used separate MLP heads for the DINO and iBOT predictions, claiming superior performance when scaling ... It is interesting to see diverging opinions in such similar papers.</p>
</blockquote>
<h3 id="koleo-regularizer">KoLeo regularizer</h3>
<p>The <strong>Koleo regularizer</strong> is an addition of DINOv2 on top of DINO, taken from the similarity search community.</p>
<blockquote>
<p><strong>Motivation:</strong> A good embedding model should use all its feature space and should push distinct images far away from each other. This property is especially well-suited for nearest-neighbor search tasks (e.g. retrieval).</p>
</blockquote>
<p>The Koleo regularizer is a simple contrastive term, that maximizes the distance between an image and its closest neighbor in the batch. Given a set of n vectors <span class="arithmatex">\((x_1, . . . , x_n)\)</span>, it is defined as:</p>
<div class="arithmatex">\[
\mathcal{L}_{koleo} = -\dfrac{1}{n} \sum_{i=1}^n \log(d_{n, i})
\]</div>
<p>where <span class="arithmatex">\(d_{n, i} = \min_{j \neq i} \lVert x_i − x_j \rVert\)</span> is the minimum distance between <span class="arithmatex">\(x_i\)</span> and any other point within the batch.</p>
<!-- > Wasn't the SwAV clustering approach enough to push embeddings away from each other? -->

<blockquote>
<p><strong>My guess</strong>: In SwAV, the repulsion is implicit. Adding the Koleo regularizer is a cheap way to add an explicit repulsive force. I guess that gave an extra control on the latent space. Moreover, the Koleo regularizer only uses samples from the current batch, so it doesn't add much compute and memory overhead.</p>
</blockquote>
<h3 id="training-stabilization">Training stabilization</h3>
<p>Large SSL models are prone to collapse during training. To stabilize the training, DINO used two opposite mechanisms:</p>
<ul>
<li>
<p><strong>Teacher centering</strong>: Subtract a running mean of past teacher predictions from the current teacher output. Centering balances the activations, preventing one dimension from dominating and reducing the risk of collapse to degenerate solutions. This has a <strong>balancing role</strong>.</p>
<blockquote>
<p><strong>Intuition</strong>: By centering the teacher targets, the predictions maintain a balanced output across dimensions, making it less likely for the model to rely too heavily on any specific direction or trivial constant output.</p>
</blockquote>
</li>
<li>
<p><strong>Softmax with Temperature</strong>: The softmax function with temperature <span class="arithmatex">\( T \)</span> is used to control the sharpness of the distribution of probabilities. It is defined as:</p>
<div class="arithmatex">\[
\text{Softmax}_i(\mathbf{z}, T) = \frac{\exp(z_i / T)}{\sum_{j=1}^{n} \exp(z_j / T)}
\]</div>
<p>For small values of <span class="arithmatex">\(T\)</span>, this has a <strong>sharpening role</strong>. It avoids the extreme situation of an uniform output distribution, i.e. all images are mapped to the same embedding.</p>
<blockquote>
<p><strong>Intuition</strong>: As <span class="arithmatex">\( T \)</span> approaches 0, the expression simplifies due to the exponential growth of the largest value of <span class="arithmatex">\( z_j / T \)</span>. The output becomes a one-hot vector, where the position corresponding to the maximum value in <span class="arithmatex">\( \mathbf{z} \)</span> is 1, and all other positions are 0.</p>
<div class="arithmatex">\[
\lim_{T \to 0} \text{Softmax}_i(\mathbf{z}, T) =
\begin{cases}
1, &amp; \text{if } i = \arg \max_k z_k \\
0, &amp; \text{otherwise}
\end{cases}
\]</div>
<table>
<thead>
<tr>
<th>Temperature value</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><span class="arithmatex">\( T &gt; 1 \)</span></strong></td>
<td>Increases the spread of the output probabilities, making them more uniform.</td>
</tr>
<tr>
<td><strong><span class="arithmatex">\( T &lt; 1 \)</span></strong> <strong>[OUR CASE]</strong></td>
<td>Sharpens the probabilities, making the largest values more dominant.</td>
</tr>
<tr>
<td><strong><span class="arithmatex">\( T = 1 \)</span></strong></td>
<td>Recovers the standard softmax function.</td>
</tr>
</tbody>
</table>
</blockquote>
</li>
</ul>
<p><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg></span> That explains the CE forward computation of the aforementioned pseudo-code:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># C: center (K)</span>
<span class="c1"># tps, tpt: student and teacher temperatures</span>

<span class="k">def</span><span class="w"> </span><span class="nf">H</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>  <span class="c1"># stop gradient</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">s</span> <span class="o">/</span> <span class="n">tps</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">((</span><span class="n">t</span> <span class="o">-</span> <span class="n">C</span><span class="p">)</span> <span class="o">/</span> <span class="n">tpt</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># center + sharpen</span>
    <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">t</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="n">s</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div>
<h3 id="swav-centering">SwAV centering</h3>
<p>The last addition of DINOv2 is the Sinkhorn-Knopp centering applied to the teacher, a training stabilization concept introduced by <strong>SwAV</strong>: Swapping Assignments between Views (another paper from Meta). SwAV is a self-supervised learning (SSL) method that employs <strong>online clustering</strong> to stabilize training and improve representation learning.</p>
<blockquote>
<p>WTF are you talking about ... What has clustering to do with visual encoders?</p>
</blockquote>
<p>Consider a dataset of images. Embed these images using a pre-trained model. Assume the embedding space is diverse enough for the points to be grouped into <span class="arithmatex">\(K\)</span> clusters <span class="arithmatex">\(C_1, C_2, ..., C_K\)</span>, representative of the dataset.</p>
<p>With fixed clusters, the cluster assignments can be treated as a <span class="arithmatex">\(K\)</span>-class classification problem. This task can be used as an SSL objective for training the model.</p>
<figure>
<p><img alt="clusters" src="../../../../images/9/clusters.jpg" width="600" /></p>
<figcaption>Even a random network tends to group similar images. This can be used to build a pseudo-classification task.</figcaption>
</figure>
<blockquote>
<p>What makes SwAV's approach different?</p>
</blockquote>
<p>Previous clustering-based SSL approaches used an offline clustering algorithm<sup id="fnref:8"><a class="footnote-ref" href="#fn:8">8</a></sup> to group samples into target clusters (e.g., K-Means). Yet, even if the objective in clustering is tractable, it does not scale well with the dataset as it requires a pass over the entire dataset to form cluster assignments that used as targets during training<sup id="fnref:9"><a class="footnote-ref" href="#fn:9">9</a></sup>. Moreover, these methods often led to degenerate solutions, where a trivial outcome is assigning all points to the same cluster. Related works employed various tricks to mitigate this issue, but these solutions were not always stable.</p>
<figure>
<p><img alt="deep-cluster-v2" src="../../../../images/9/deep-cluster-v2.png" width="500" /></p>
<figcaption>Before SwAV, clustering-based SSL approaches relied on offline clustering steps. Here is the example from DeepCluster-v2.</figcaption>
</figure>
<p>SwAV addresses these challenges by (1) learning centroids as model parameters and (2) enforcing an equipartition constraint, ensuring an even distribution of samples across the <span class="arithmatex">\(K\)</span> clusters. In practice, SwAV maps a batch of <span class="arithmatex">\(B\)</span> samples to <span class="arithmatex">\(K\)</span> clusters and applies the equipartition constraint using a cost matrix<sup id="fnref2:9"><a class="footnote-ref" href="#fn:9">9</a></sup>. This setup resembles an optimal transport problem, which is relaxed for faster computation. The resulting loss function is:</p>
<div class="arithmatex">\[
\max_{Q \in \mathcal{Q}} \ \text{Tr}(Q^\top C^\top Z) + \varepsilon H(Q)
\]</div>
<blockquote>
<p>Intuitively, aligning the predicted clusters <span class="arithmatex">\(C^\top Z\)</span> with the label clusters <span class="arithmatex">\(Q\)</span> maximizes the function. The entropy term <span class="arithmatex">\(H(Q)\)</span> acts as regularization to enforce equipartition.</p>
</blockquote>
<p>This relaxed problem has a fast, computable solution. The target clusters are determined as:</p>
<div class="arithmatex">\[
Q^* = \text{Diag}(u) \exp\left(\frac{C^\top Z}{\varepsilon}\right) \text{Diag}(v)
\]</div>
<p>where <span class="arithmatex">\(u\)</span> and <span class="arithmatex">\(v\)</span> are renormalization vectors in <span class="arithmatex">\(\mathbb{R}^K\)</span> and <span class="arithmatex">\(\mathbb{R}^B\)</span>, respectively. These vectors are computed iteratively using the <strong>Sinkhorn-Knopp</strong> algorithm, which lends its name to the centering technique.</p>
<div class="admonition quote">
<p class="admonition-title">More about the equivalence to an optimal transport problem ...</p>
<p>The DINOv2 method references SwAV, which in turn points to the paper "Self-labelling via simultaneous clustering and representation learning"<sup id="fnref:10"><a class="footnote-ref" href="#fn:10">10</a></sup>. This paper provides a clear explanation of the motivation behind using the SK algorithm, making it a recommended read for a deeper understanding.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Let's summarize</p>
<p>SwAV jointly learns target clusters and trains the model to assign features to clusters. The target clusters are computed online using the SK algorithm from the features and the learned clusters.</p>
</div>
<blockquote>
<p>Enough theory. How is it implemented?</p>
</blockquote>
<p>SwAV uses a finite set of prototypes (<span class="arithmatex">\(C\)</span>), learnable parameters that act as centroids summarizing the feature space. These prototypes project features (<span class="arithmatex">\(Z\)</span>) into codes (<span class="arithmatex">\(Q\)</span>), representing soft cluster assignments. The codes indicate the similarity between features and prototypes (as shown in the expression for <span class="arithmatex">\(Q^*\)</span>).</p>
<p>The <strong>core idea</strong> is to predict the code (cluster assignment) of one augmented view using the features from another view (that is the <strong>swap</strong> of <strong>Sw</strong>AV). This ensures that representations are invariant to augmentations while being semantically meaningful.</p>
<div class="admonition quote">
<p><img alt="swav" src="../../../../images/9/swav.png" /></p>
</div>
<blockquote>
<p>SwAV Loss: How Does It Work?</p>
</blockquote>
<p>SwAV aligns cluster assignments (codes) between views rather than comparing raw features. Its loss function is:</p>
<div class="arithmatex">\[
\mathcal{L} = H(z^{t}_1, q^{t}_2) / 2 + H(z^{t}_2, q^{t}_1) / 2,
\]</div>
<p>where: <span class="arithmatex">\(H\)</span> is the cross-entropy loss, <span class="arithmatex">\(z^{t}\)</span> are the features, <span class="arithmatex">\(q^{t}\)</span> are cluster assignments (codes) from the features.</p>
<p>In simple terms, the first term aligns the teacher’s embedding of view 1 (<span class="arithmatex">\(z_1^{t}\)</span>) with the cluster assignment (<span class="arithmatex">\(q_2^{s}\)</span>) of view 2. The second term swaps the roles of view 1 and view 2. This swapping mechanism maps both views to consistent clusters, enforcing augmentation invariance.</p>
<details class="quote">
<summary>SwAV Pseudo-code for the curious ones (from the paper<sup id="fnref3:9"><a class="footnote-ref" href="#fn:9">9</a></sup>)</summary>
<div class="highlight"><pre><span></span><code><span class="c1"># C: prototypes (DxK)</span>
<span class="c1"># model: convnet + projection head</span>
<span class="c1"># temp: temperature</span>
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span> <span class="c1"># load a batch x with B samples</span>
    <span class="n">x_t</span> <span class="o">=</span> <span class="n">t</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># t is a random augmentation</span>
    <span class="n">x_s</span> <span class="o">=</span> <span class="n">s</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># s is a another random augmentation</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">cat</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">x_s</span><span class="p">))</span> <span class="c1"># embeddings: 2BxD</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">mm</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span> <span class="c1"># prototype scores: 2BxK</span>
    <span class="n">scores_t</span> <span class="o">=</span> <span class="n">scores</span><span class="p">[:</span><span class="n">B</span><span class="p">]</span>
    <span class="n">scores_s</span> <span class="o">=</span> <span class="n">scores</span><span class="p">[</span><span class="n">B</span><span class="p">:]</span>

    <span class="c1"># compute assignments</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">q_t</span> <span class="o">=</span> <span class="n">sinkhorn</span><span class="p">(</span><span class="n">scores_t</span><span class="p">)</span>
        <span class="n">q_s</span> <span class="o">=</span> <span class="n">sinkhorn</span><span class="p">(</span><span class="n">scores_s</span><span class="p">)</span>

    <span class="c1"># convert scores to probabilities</span>
    <span class="n">p_t</span> <span class="o">=</span> <span class="n">Softmax</span><span class="p">(</span><span class="n">scores_t</span> <span class="o">/</span> <span class="n">temp</span><span class="p">)</span>
    <span class="n">p_s</span> <span class="o">=</span> <span class="n">Softmax</span><span class="p">(</span><span class="n">scores_s</span> <span class="o">/</span> <span class="n">temp</span><span class="p">)</span>

    <span class="c1"># swap prediction problem</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">mean</span><span class="p">(</span><span class="n">q_t</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="n">p_s</span><span class="p">)</span> <span class="o">+</span> <span class="n">q_s</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="n">p_t</span><span class="p">))</span>

    <span class="c1"># SGD update: network and prototypes</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">update</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
    <span class="n">update</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>

    <span class="c1"># normalize prototypes</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">normalize</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Sinkhorn-Knopp</span>
<span class="k">def</span><span class="w"> </span><span class="nf">sinkhorn</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">niters</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="n">scores</span> <span class="o">/</span> <span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
    <span class="n">Q</span> <span class="o">/=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>
    <span class="n">K</span><span class="p">,</span> <span class="n">B</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">u</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">),</span> <span class="n">ones</span><span class="p">(</span><span class="n">K</span><span class="p">)</span> <span class="o">/</span> <span class="n">K</span><span class="p">,</span> <span class="n">ones</span><span class="p">(</span><span class="n">B</span><span class="p">)</span> <span class="o">/</span> <span class="n">B</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">niters</span><span class="p">):</span>
        <span class="n">u</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">Q</span> <span class="o">*=</span> <span class="p">(</span><span class="n">r</span> <span class="o">/</span> <span class="n">u</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">Q</span> <span class="o">*=</span> <span class="p">(</span><span class="n">c</span> <span class="o">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">Q</span> <span class="o">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
</code></pre></div>
</details>
<div class="admonition note">
<p class="admonition-title">To summarize SwAv ...</p>
<ol>
<li><strong>Prototypes Summarize the Dataset</strong>: They serve as "abstract negatives," reducing the need for explicit pairwise comparisons.</li>
<li><strong>Cluster Diversity is Loss-driven</strong>: Identical prototypes fail to minimize the loss, driving specialization across clusters.</li>
<li>
<p><strong>Sinkhorn Balancing Ensures Stability</strong>: SK ensures even cluster usage, avoiding trivial solutions.</p>
<blockquote>
<p><strong>How many prototypes?</strong> The number isn't critical. A good rule of thumb is to use about 10x the number of classes.</p>
</blockquote>
</li>
</ol>
</div>
<h3 id="high-resolution-training-phase">High-resolution training phase</h3>
<p>Training models that are pixel-precise is important for some pixel-precise downstream tasks, such as segmentation. This requires <strong>high-quality images</strong> during training, otherwise small objects tend to disappear. Sadly, this is more time and memory-demanding.</p>
<p>Similarly to what we saw in the posts about <a href="../../../../2024/02/13/overfit2-from-ddpms-to-stable-diffusion-xl/">Stable Diffusion</a>, DINOv2 authors mitigate this issue by finishing the pre-training on a higher quality dataset. Thus, the model benefits from the quality of the dataset, at a smaller computational cost.</p>
<h2 id="implementation-details">Implementation details</h2>
<h3 id="dataset-curation">Dataset curation</h3>
<p>One of the biggest strengths of SSL is that <strong>it enables training on unsupervised datasets</strong>, providing access to datasets far larger than typical supervised ones.</p>
<p>With DINOv2, the authors created a processing pipeline for a 142M-image dataset, curated from images crawled from the open web. For context, ImageNet-22k, a common supervised pre-training dataset, contains only 13,673,551 samples.</p>
<ul>
<li><strong>Embedding</strong>: A ViT-H/16 pre-trained on ImageNet-22k extracts visual features.</li>
<li><strong>Deduplication</strong>: Near-duplicates are removed and the dataset is rebalanced using a Meta copy detection pipeline<sup id="fnref:6"><a class="footnote-ref" href="#fn:6">6</a></sup>.</li>
<li><strong>Retrieval</strong>: For each image in the curated dataset, the <span class="arithmatex">\(N\sim4\)</span> closest uncurated images are added to the final dataset.</li>
</ul>
<p>To compute similarities between queries and uncurated images, DINOv2's authors used <a href="https://github.com/facebookresearch/faiss">FAISS</a> by Meta, a library for efficient similarity search and clustering of dense vectors.</p>
<div class="admonition quote">
<p><img alt="dataproc_pipeline" src="../../../../images/9/data_proc_pipeline.png" /></p>
</div>
<h3 id="training-details">Training details</h3>
<p>In addition to its complex training process, DINOv2 benefits from implementation tips, to maximize training efficiency.</p>
<ul>
<li><strong>NN architecture optimization</strong>: Hidden-layer dimension carefully chosen to maximize GPU usage (depends on the GPU memory specs)</li>
<li><strong>Sequence packing</strong>: DINO uses crops of images during training. This results in varying sequence input sizes. To maximize the training efficiency, the sequence are packed (concatenated) together. A block-wise mask is then added in the attention mechanism to ensure sequence independence. This technique is similar to NLP packing techniques.</li>
<li><strong>Efficient stochastic depth</strong>: When training in a MAE manner, time is lost computing masked tokens. Their implementation skips these calculations, thanks to kernel operation fusion. The higher the drop rate, the bigger the acceleration.</li>
<li><strong>Fully-Sharded Data Parallel (FSDP)</strong>: Parallelized training across multiple GPUs, using a mix of float32/16 reduce operations to reduce the communication overhead between GPUs (more on training parallelization in future posts ...).</li>
</ul>
<h2 id="results">Results</h2>
<p>I won't dive into the benchmark results of DINOv2, the paper does it much better. Instead, I would like to highlight the versatility of DINOv2 for downstream tasks and give an example of industrial usage on other datasets.</p>
<p><strong>Out-of-the-box Segmentation</strong></p>
<p>One striking example of DINOv2’s capabilities is its ability to perform object segmentation directly from embeddings. By simply applying Principal Component Analysis (PCA) to the feature maps and thresholding the leading principal component, DINOv2 can segment the main object in the frame with remarkable accuracy. This approach was showcased in the paper using a video of a dog, where the model was able to segment the dog in each frame with high precision, purely based on its learned embeddings.</p>
<p>By visualizing the top three PCA components as an RGB image, they also observed that similar regions in the image have similar colors. This consistency in color highlights the model's ability to cluster similar patches together, indicating that segmentation out-of-the-box with a simple linear layer should be possible and effective.</p>
<figure>
<p><img alt="gif" src="https://cdn.prod.website-files.com/60d1a7f6aeb33c5c595468b4/655e8059ed48672faddad0c0_dino1.gif" /></p>
<figcaption>Despite training without supervision, DINOv2's embeddings allow out-of-the-box segmentation with a simple PCA.</figcaption>
</figure>
<p><strong>Patch concept understanding</strong></p>
<p>DINOv2 goes beyond simple image classification and segmentation. It shows a deep understanding of image concepts and relationships. For instance, the model can match patches between different images based on semantic similarity. An example from the paper demonstrates the model correctly mapping the wings of an airplane to the wings of a dinosaur. This level of conceptual understanding shows the robustness and depth of the embeddings learned by DINOv2.</p>
<figure>
<p><img alt="patch matching" src="../../../../images/9/patch-matching.png" width="500" /></p>
</figure>
<p><strong>Community Adoption</strong></p>
<p>The power and versatility of DINOv2 have led to its rapid adoption in various domains. One notable example is bio-medical imaging. <a href="https://github.com/bioptimus/releases/tree/main/models/h-optimus/v0?utm_source=owkin&amp;utm_medium=referral&amp;utm_campaign=h-bioptimus-o">H-Optimus-0</a> is a model trained on Whole Slide Images (WSI), released by Bioptimus. It achieves top-1 on most downstream histopathology-related tasks. Microsoft also released <a href="https://huggingface.co/microsoft/rad-dino">RAD-DINO</a> a vision transformer model trained to encode chest X-rays. RAD-DINO is one of the most downloaded feature extraction model on HuggingFace.</p>
<p>These applications underscore the robustness of DINOv2 and its suitability for pre-training on large, diverse datasets.</p>
<h2 id="limitations-and-extensions">Limitations and extensions</h2>
<p>A recent paper by the same authors, titled "<a href="https://arxiv.org/abs/2309.16588">Vision Transformers Need Registers (2023)</a>"<sup id="fnref:3"><a class="footnote-ref" href="#fn:3">3</a></sup>, highlights that ViTs tend to store information in certain patches, causing discrepancies in the attention maps. Adding registers helps the network smooth these maps effectively.</p>
<figure>
<p><img alt="registers" src="../../../../images/9/registers.png" width="600" /></p>
</figure>
<hr />
<div class="admonition note">
<p class="admonition-title">Conclusion</p>
<p>To sum up, DINOv2 is an SSL training recipe for large image encoders, sitting at the crossroads of contrastive and clustering-based methods. Its rapid adoption in the industry highlights its promising potential to train general base models for downstream tasks.</p>
<p>It will be interesting to see how future SSL methods evolve — perhaps the next step will focus on simplifying the framework, like DeepMind did between AlphaFoldv2 and v3. If interested, I wrote a series about AFv2, check it out.</p>
<p>Thank you for reading this post! I hope you enjoyed it. If so, feel free to share it and connect 😊</p>
</div>
<h2 id="references">References</h2>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p><strong>DINO</strong>: Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., &amp; Joulin, A. (2021). <a href="https://arxiv.org/abs/2104.14294">Emerging properties in self-supervised vision transformers</a>. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 9650-9660)&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p><strong>DINOv2</strong>: Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., ... &amp; Bojanowski, P. (2023). <a href="https://arxiv.org/pdf/2304.07193">Dinov2: Learning robust visual features without supervision</a>. arXiv preprint arXiv:2304.07193.&#160;<a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:3">
<p>Darcet, T., Oquab, M., Mairal, J., &amp; Bojanowski, P. (2023). <a href="https://arxiv.org/abs/2309.16588">Vision transformers need registers</a>. arXiv preprint arXiv:2309.16588.&#160;<a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:4">
<p><strong>MAE</strong>: He, K., Chen, X., Xie, S., Li, Y., Dollár, P., &amp; Girshick, R. (2022). <a href="https://arxiv.org/abs/2111.06377">Masked autoencoders are scalable vision learners</a>. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 16000-16009).&#160;<a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
<li id="fn:5">
<p><strong>iBOT</strong>: Zhou, J., Wei, C., Wang, H., Shen, W., Xie, C., Yuille, A., &amp; Kong, T. (2021). <a href="https://arxiv.org/abs/2111.07832">ibot: Image bert pre-training with online tokenizer</a>. arXiv preprint arXiv:2111.07832.&#160;<a class="footnote-backref" href="#fnref:5" title="Jump back to footnote 5 in the text">&#8617;</a></p>
</li>
<li id="fn:6">
<p>Pizzi, E., Roy, S. D., Ravindra, S. N., Goyal, P., &amp; Douze, M. (2022). <a href="https://arxiv.org/abs/2202.10261">A self-supervised descriptor for image copy detection</a>. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 14532-14542).&#160;<a class="footnote-backref" href="#fnref:6" title="Jump back to footnote 6 in the text">&#8617;</a></p>
</li>
<li id="fn:7">
<p><strong>SimCLR</strong>: Chen, T., Kornblith, S., Norouzi, M., &amp; Hinton, G. (2020, November). <a href="https://arxiv.org/abs/2002.05709">A simple framework for contrastive learning of visual representations</a>. In International conference on machine learning (pp. 1597-1607). PMLR.&#160;<a class="footnote-backref" href="#fnref:7" title="Jump back to footnote 7 in the text">&#8617;</a></p>
</li>
<li id="fn:8">
<p><strong>DeepCluster-v2</strong>: Caron, M., Bojanowski, P., Joulin, A., &amp; Douze, M. (2018). Deep clustering for unsupervised learning of visual features. In Proceedings of the European conference on computer vision (ECCV) (pp. 132-149).&#160;<a class="footnote-backref" href="#fnref:8" title="Jump back to footnote 8 in the text">&#8617;</a></p>
</li>
<li id="fn:9">
<p><strong>SwAV</strong>: Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., &amp; Joulin, A. (2020). <a href="https://arxiv.org/abs/2006.09882">Unsupervised learning of visual features by contrasting cluster assignments</a>. Advances in neural information processing systems, 33, 9912-9924.&#160;<a class="footnote-backref" href="#fnref:9" title="Jump back to footnote 9 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:9" title="Jump back to footnote 9 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:9" title="Jump back to footnote 9 in the text">&#8617;</a></p>
</li>
<li id="fn:10">
<p>Asano, Y. M., Rupprecht, C., &amp; Vedaldi, A. (2019). <a href="https://arxiv.org/abs/1911.05371">Self-labelling via simultaneous clustering and representation learning</a>. arXiv preprint arXiv:1911.05371.&#160;<a class="footnote-backref" href="#fnref:10" title="Jump back to footnote 10 in the text">&#8617;</a></p>
</li>
</ol>
</div>







  
  




  



      
    </article>
  </div>

          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/LouisStefanuto" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.linkedin.com/in/louis-stefanuto/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../../../..", "features": ["navigation.tabs", "navigation.expand"], "search": "../../../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../../../assets/javascripts/bundle.92b07e13.min.js"></script>
      
        <script src="../../../../../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>