
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Personal website of Louis Stefanuto">
      
      
      
      
        <link rel="prev" href="../../../01/13/overfit9-dinov2-explained-learning-robust-visual-features-without-supervision/">
      
      
        <link rel="next" href="../../../06/25/overfit11-v-jepa-2/">
      
      
      <link rel="icon" href="../../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.18">
    
    
      
        <title>Overfit#10: DeepSeek-R1 - Louis Stefanuto</title>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/stylesheets/main.7e37652d.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#overfit10-deepseek-r1" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../../.." title="Louis Stefanuto" class="md-header__button md-logo" aria-label="Louis Stefanuto" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Louis Stefanuto
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Overfit#10: DeepSeek-R1
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
    
      <div class="md-header__source">
        <a href="https://github.com/LouisStefanuto" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    Github
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../about/" class="md-tabs__link">
        
  
  
    
  
  About

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../../../" class="md-tabs__link">
          
  
  
  Posts

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../../.." title="Louis Stefanuto" class="md-nav__button md-logo" aria-label="Louis Stefanuto" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Louis Stefanuto
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/LouisStefanuto" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    Github
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../about/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    About
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Posts
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Posts
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      <a href="../../../../" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Index
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Post series
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            Post series
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/alphafold/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AlphaFold
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/chemistry/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Chemistry
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/computer-vision/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Computer Vision
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/diffusion-models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Diffusion models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/drug-discovery/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Drug Discovery
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/embedding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Embedding
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/llm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LLM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/reinforcement-learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Reinforcement Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/robotics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Robotics
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/ssl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SSL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/video/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Video
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#deepseek-r1-zero" class="md-nav__link">
    <span class="md-ellipsis">
      DeepSeek-R1 Zero
    </span>
  </a>
  
    <nav class="md-nav" aria-label="DeepSeek-R1 Zero">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rl-is-all-you-need" class="md-nav__link">
    <span class="md-ellipsis">
      RL Is All You Need
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grpo" class="md-nav__link">
    <span class="md-ellipsis">
      GRPO
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#deepseek-r1" class="md-nav__link">
    <span class="md-ellipsis">
      DeepSeek-R1
    </span>
  </a>
  
    <nav class="md-nav" aria-label="DeepSeek-R1">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sft-data-collection" class="md-nav__link">
    <span class="md-ellipsis">
      SFT data collection
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SFT data collection">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#reasoning-data" class="md-nav__link">
    <span class="md-ellipsis">
      Reasoning data
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#non-reasoning-data" class="md-nav__link">
    <span class="md-ellipsis">
      Non-reasoning data
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-training-sft-rl" class="md-nav__link">
    <span class="md-ellipsis">
      Model training (SFT + RL)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      References
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
  <div class="md-content md-content--post" data-md-component="content">
    <div class="md-sidebar md-sidebar--post" data-md-component="sidebar" data-md-type="navigation">
      <div class="md-sidebar__scrollwrap">
        <div class="md-sidebar__inner md-post">
          <nav class="md-nav md-nav--primary">
            <div class="md-post__back">
              <div class="md-nav__title md-nav__container">
                <a href="../../../../" class="md-nav__link">
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
                  <span class="md-ellipsis">
                    Back to index
                  </span>
                </a>
              </div>
            </div>
            
              <div class="md-post__authors md-typeset">
                
                  <div class="md-profile md-post__profile">
                    <span class="md-author md-author--long">
                      <img src="https://github.com/LouisStefanuto.png" alt="Louis Stefanuto">
                    </span>
                    <span class="md-profile__description">
                      <strong>
                        
                          Louis Stefanuto
                        
                      </strong>
                      <br>
                      Creator
                    </span>
                  </div>
                
              </div>
            
            <ul class="md-post__meta md-nav__list">
              <li class="md-nav__item md-nav__item--section">
                <div class="md-post__title">
                  <span class="md-ellipsis">
                    Metadata
                  </span>
                </div>
                <nav class="md-nav">
                  <ul class="md-nav__list">
                    <li class="md-nav__item">
                      <div class="md-nav__link">
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 19H5V8h14m-3-7v2H8V1H6v2H5c-1.11 0-2 .89-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2h-1V1m-1 11h-5v5h5z"/></svg>
                        <time datetime="2025-02-01 00:00:00+00:00" class="md-ellipsis">February 1, 2025</time>
                      </div>
                    </li>
                    
                    
                      <li class="md-nav__item">
                        <div class="md-nav__link">
                          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9 3v15h3V3zm3 2 4 13 3-1-4-13zM5 5v13h3V5zM3 19v2h18v-2z"/></svg>
                          <span class="md-ellipsis">
                            in
                            
                              <a href="../../../../category/llm/">LLM</a>, 
                              <a href="../../../../category/reinforcement-learning/">Reinforcement Learning</a></span>
                        </div>
                      </li>
                    
                    
                      
                      <li class="md-nav__item">
                        <div class="md-nav__link">
                          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 20a8 8 0 0 0 8-8 8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8m0-18a10 10 0 0 1 10 10 10 10 0 0 1-10 10C6.47 22 2 17.5 2 12A10 10 0 0 1 12 2m.5 5v5.25l4.5 2.67-.75 1.23L11 13V7z"/></svg>
                          <span class="md-ellipsis">
                            
                              8 min read
                            
                          </span>
                        </div>
                      </li>
                    
                  </ul>
                </nav>
              </li>
            </ul>
            
          </nav>
          
        </div>
      </div>
    </div>
    <article class="md-content__inner md-typeset">
      
        
  



  
  


<h1 id="overfit10-deepseek-r1"><strong>Overfit#10:</strong> DeepSeek-R1</h1>
<p><a href="./"><img alt="main deepseek r1" src="../../../../images/10/main.jpg" /></a></p>
<!-- more -->

<p>On January 20, 2025, DeepSeek introduced <a href="https://github.com/deepseek-ai/DeepSeek-R1">a new series of reasoning models</a>: <strong>DeepSeek-R1</strong> and <strong>DeepSeek-R1 Zero</strong><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup>. Alongside these, they open-sourced six distilled models (1.5B, 7B, 8B, 14B, 32B, and 70B), derived from DeepSeek-R1 and built on Qwen and Llama.</p>
<p>What sets DeepSeek apart is its innovative post-training approach, enhancing reasoning capabilities while maintaining model helpfulness and readability. But what’s behind this new paradigm? Let's find out!</p>
<figure>
<p><img alt="bench" src="../../../../images/10/bench.png" width="500" /></p>
<figcaption>From DeepSeek-R1's paper [1].</figcaption>
</figure>
<h2 id="deepseek-r1-zero">DeepSeek-R1 Zero</h2>
<h3 id="rl-is-all-you-need">RL Is All You Need</h3>
<p>One of DeepSeek-R1 Zero's key contributions is its <strong>direct application of Reinforcement Learning</strong> (RL) to train reasoning capabilities. Instead of relying on Supervised Fine-Tuning (SFT) with human-annotated reasoning examples, DeepSeek-R1 Zero optimizes reasoning purely through an RL-based reward system.</p>
<blockquote>
<p>Why RL instead of SFT?</p>
</blockquote>
<p>Training models on high-quality reasoning datasets via SFT is effective but comes with limitations:</p>
<ul>
<li><strong>Data collection is expensive</strong>: Human annotations for reasoning (e.g., Chain of Thought (CoT) examples) require significant effort and cost.</li>
<li><strong>Scalability issues</strong>: Expanding curated datasets to improve generalization is difficult.</li>
<li><strong>Overfitting to human patterns</strong>: Models trained on human demonstrations tend to imitate reasoning rather than discover optimal strategies.</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This aligns with a recent DeepMind's study, <a href="https://arxiv.org/abs/2501.17161">“SFT Memorizes, RL Generalizes” (2025)</a> which suggests that RL-trained models tend to generalize better than those trained via SFT alone.</p>
</div>
<blockquote>
<p>RL requires an environment to interact with. What does a reasoning training environment look like?</p>
</blockquote>
<p>DeepSeek-R1 Zero uses two key reward mechanisms:</p>
<ul>
<li><strong>Format Rewards</strong>: Encourage structured reasoning by introducing <code>&lt;think&gt;</code> and <code>&lt;/think&gt;</code> tokens, prompting the model to generate explicit reasoning steps.</li>
<li><strong>Accuracy Rewards</strong>: Evaluate correctness using deterministic tasks, such as math problems with fixed solutions or programming challenges with unit tests.</li>
</ul>
<p>By maximizing its rewards over time, the model learns to refine its reasoning strategies. Interestingly, the model finds that "thinking" with longer responses leads to better rewards. The plot below shows how the response length grows with the number of RL training steps. This is a similar observation as scaling <strong>test-time compute</strong> observed by OpenAI.</p>
<div class="admonition quote">
<p><figure markdown>
<img alt="bench" src="../../../../images/10/response-length.png" width="600" />
<figcaption>From DeepSeek-R1's paper [1].</figcaption>
</figure></p>
</div>
<p>Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of <strong>majority voting</strong>, as depicted in this second plot.</p>
<div class="admonition quote">
<p><figure markdown>
<img alt="bench" src="../../../../images/10/aime-bench.png" width="600" />
<figcaption>From DeepSeek-R1's paper [1].</figcaption>
</figure></p>
</div>
<h3 id="grpo">GRPO</h3>
<p>Before introducing R1, I wanted to take a moment to highlight <strong>Group Relative Policy Optimization (GRPO)</strong>, the policy-based RL algorithm behind DeepSeek-R1 Zero. GRPO was introduced by DeepSeek a few months earlier in <strong>DeepSeekMath</strong><sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup>. To understand its novelty, let’s first look at PPO.</p>
<hr />
<p><strong>Proximal Policy Optimization (PPO)</strong> is the most widely used RL algorithm for aligning LLMs. As a policy-based RL method, it directly learns a policy (in this case, the LLM) to maximize rewards, which are typically computed by a <strong>reward model</strong> trained on human preferences. Additionally, PPO incorporates a value model (the critic) to reduce gradient variance. The <strong>advantage function</strong> in PPO, defined as: <span class="arithmatex">\(A_t = r_t - v_{\pi_\theta(q,o&lt;t)}\)</span> measures how much better (or worse) a reward is compared to the expected return.</p>
<div class="arithmatex">\[
L_{\text{PPO}}(\theta) = \mathbb{E} \left[ \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \min \left( \frac{\pi_\theta}{\pi_{old}} A_t, \text{clip}(\frac{\pi_\theta}{\pi_{old}}, 1 - \epsilon, 1 + \epsilon) A_t \right) \right]
\]</div>
<p><figure markdown><br />
<img alt="bench" src="../../../../images/10/ppo-grpo.png" width="600" />
</figure>  </p>
<blockquote>
<p>What is the issue with PPO?</p>
</blockquote>
<p>PPO relies on a value model <span class="arithmatex">\(v_{\pi_{\theta}}\)</span> (the critic) which must be trained alongside the policy, adding computational overhead. <strong>GRPO</strong> addresses this by replacing the trainable value function with the normalized mean reward of a set of sampled answers.  In GRPO, the advantage is how good a reward is <strong>relative to the other</strong> answers:  <span class="arithmatex">\(\hat{A}_{i,t} = \tilde{r}_i = \frac{r_i - \text{mean}(\mathbf{r})}{\text{std}(\mathbf{r})}\)</span></p>
<p><strong>For each question</strong> <span class="arithmatex">\(q\)</span>, GRPO samples a group of <span class="arithmatex">\(G\)</span> outputs <span class="arithmatex">\((o_1, o_2,··· , o_G)\)</span> from the old policy <span class="arithmatex">\(\pi_{old}\)</span> and optimizes the policy model using the following objective<sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">1</a></sup>:  </p>
<div class="arithmatex">\[
L_{\text{GRPO}}(\theta) = \mathbb{E} \left[ \frac{1}{G} \sum_{i=1}^{G} \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \left( \min \left( \frac{\pi_\theta}{\pi_{old}} A_{i,t}, \text{clip}(\frac{\pi_\theta}{\pi_{old}}, 1 - \epsilon, 1 + \epsilon) A_{i,t} \right) - \beta D_{KL}(\pi_\theta || \pi_{ref}) \right) \right]
\]</div>
<blockquote>
<p><strong>Note 1</strong>: The advantages <span class="arithmatex">\(A_{i,t}\)</span> are <strong>independent of the token position</strong> <span class="arithmatex">\(t\)</span>. Every token of an output <span class="arithmatex">\(o_i\)</span> receives the same advantage value, even though the reward is typically obtained at the last token.  </p>
<p><strong>Note 2</strong>: The GRPO paper<sup id="fnref2:2"><a class="footnote-ref" href="#fn:2">2</a></sup> explores alternatives to compute advantages. However, DeepSeek-R1 adopts a <strong>simpler approach</strong>, where the rewards are only assigned at the end of the sequence. The loss expression simplifies a bit (no more summation over <span class="arithmatex">\(t\)</span>).</p>
<div class="arithmatex">\[
L_{\text{GRPO}}(\theta) = \mathbb{E} \left[ \frac{1}{G} \sum_{i=1}^{G} \left( \min \left( \frac{\pi_\theta}{\pi_{old}} A_i, \text{clip}(\frac{\pi_\theta}{\pi_{old}}, 1 - \epsilon, 1 + \epsilon) A_i \right) - \beta D_{KL}(\pi_\theta || \pi_{ref}) \right) \right]
\]</div>
</blockquote>
<p>There is one last concept we didn't cover: the <strong>reference model</strong> <span class="arithmatex">\(\pi_{ref}\)</span>. One common issue of RL is reward hacking where the model exploits flaws in the reward model rather than genuinely improving. To prevent this, PPO and GRPO constrain the RL model to stay "close" from the pre-trained/SFT reference. In PPO, this is done at the token level (see InstructGPT<sup id="fnref:3"><a class="footnote-ref" href="#fn:3">3</a></sup>, page 9, section 3.5). In GRPO, they use a KL-divergence that they consider easier. This explains the difference of the "KL arrows" in the PPO vs GRPO picture above.</p>
<h2 id="deepseek-r1">DeepSeek-R1</h2>
<p>Despite its strong reasoning capabilities, DeepSeek-R1 Zero has <strong>notable drawbacks</strong>. Due to its "no human in the loop" training process, the model frequently switches languages while reasoning, markdown outputs are often incorrect, and responses are sometimes unhelpful or misaligned with human expectations.</p>
<p><strong>DeepSeek-R1</strong> tries to address these weaknesses by first finetuning a pretrained checkpoint <em>V3-Base</em> (step 3c) on a high-quality SFT dataset (steps 1/2/3a/3b), and then aligning the model with a mixture of RHLF/R1-Zero RL pipeline (step 4).</p>
<figure>
<p><img alt="bench" src="../../../../images/10/deepseekr1.jpg" width="500" /></p>
<figcaption>Brilliant figure from Harris Chan with comments.</figcaption>
</figure>
<h3 id="sft-data-collection">SFT data collection</h3>
<h4 id="reasoning-data">Reasoning data</h4>
<p>The first step is to build a "poor" reasoning model to generate lots of CoTs. The recipe is a refined version of the R1-Zero's training recipe. Starting from a pre-trained base model <em>V3-Base</em>:</p>
<ul>
<li><strong>Step 1 - Cold Start</strong>: Finetune on a small amount of CoTs. This short SFT phase helps mitigate the unstable cold start phase of RL training.</li>
<li><strong>Step 2 - RORL</strong>: Learn reasoning through Reasoning-oriented Reinforcement Learning (RORL). It is essentially the R1-Zero training recipe (GRPO) with an extra reward for language consistency.</li>
</ul>
<div class="admonition quote">
<p><figure markdown>
<img alt="bench" src="../../../../images/10/data_reasoning.jpg" width="300" />
</figure></p>
</div>
<p>We now have a "poor" reasoning model. We can utilize it to generate lots of CoTs.</p>
<ul>
<li><strong>Step 3a: Rejection Sampling and Supervised Fine-Tuning</strong>: Given a set of CoT prompts, sample multiple CoT answers with the RORL model. The answers are then assessed either by correctness criteria or LLM as a judge. Only the best answers are collected. Additional filtering is performed to remove responses with mixed languages or long verbose paragraphs for instance. <span class="arithmatex">\(\rightarrow\)</span> This gives the <strong>600k reasoning samples</strong> dataset.</li>
</ul>
<h4 id="non-reasoning-data">Non-reasoning data</h4>
<div class="admonition quote">
<p><figure markdown>
<img alt="bench" src="../../../../images/10/sft_data_collection.png" width="300" />
</figure></p>
</div>
<p>To get a helpful chat model, we need some <strong>conversational</strong> examples. A complementary non-reasoning dataset is thus built by merging two sources. The first half consists of writing, factual QA examples sampled from DeepSeek-V3 (Step 3b). The second half is a repurposed subset of the DeepSeek-V3's SFT dataset from previous work, which provides additional supervised fine-tuning examples.</p>
<p><span class="arithmatex">\(\rightarrow\)</span> This gives the <strong>200k non-reasoning samples</strong> dataset.</p>
<h3 id="model-training-sft-rl">Model training (SFT + RL)</h3>
<blockquote>
<p>We have the data. Time to train the final model!</p>
</blockquote>
<p>DeepSeek-R1's training procedure follows the SFT + RL framework. The SFT phase (step 3c) is standard finetuning over the 800k dataset. The RL phase is a mix of DPO for human alignment and of R1-Zero RL technique for reasoning. The result is <strong>DeepSeekR1</strong>, the model that made the headlines a few weeks ago.</p>
<div class="admonition quote">
<p><figure markdown>
<img alt="bench" src="../../../../images/10/final.png" width="300" />
</figure></p>
</div>
<p>Aside from 671B R1 model, DeepSeek additionally released <strong>smaller models</strong>. Note that these models are "only" distilled Llama and Qwen2 checkpoints, fine-tuned on the 800k high-quality dataset of step 3. <strong>No RL is used</strong>.</p>
<p>One may ask why using SFT over RL after all the promising results we mentioned above. Based on their experiments, they claim that smaller models benefit more from distillation than from going through the whole R1 pipeline.</p>
<div class="admonition quote">
<p><figure markdown>
<img alt="bench" src="../../../../images/10/distillation.png" width="300" />
</figure></p>
</div>
<hr />
<div class="admonition note">
<p class="admonition-title">Conclusion</p>
<p>To conclude, this is a truly interesting paper from DeeSeek, with lots of engineering involved. I am eager to see what they are cooking up for their next model iterations.</p>
<p>I hope you enjoyed reading this technical deep dive. If so, feel free to share it and connect 😊</p>
</div>
<h2 id="references">References</h2>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p><strong>DeepSeek-R1</strong>: Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., ... &amp; He, Y. (2025). Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. <a href="https://arxiv.org/abs/2501.12948">arXiv preprint arXiv:2501.12948</a>.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p><strong>DeepSeekMath</strong>: Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., ... &amp; Guo, D. (2024). Deepseekmath: Pushing the limits of mathematical reasoning in open language models. <a href="https://arxiv.org/abs/2402.03300">arXiv preprint arXiv:2402.03300</a>.&#160;<a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:2" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:3">
<p><strong>InstructGPT from OpenAI</strong>: Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., ... &amp; Lowe, R. (2022). <a href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human feedback</a>. Advances in neural information processing systems, 35, 27730-27744.&#160;<a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
</ol>
</div>







  
  




  



      
    </article>
  </div>

          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/LouisStefanuto" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.linkedin.com/in/louis-stefanuto/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../../../..", "features": ["navigation.tabs", "navigation.expand"], "search": "../../../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../../../assets/javascripts/bundle.92b07e13.min.js"></script>
      
        <script src="../../../../../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>