
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Personal website of Louis Stefanuto">
      
      
      
      
        <link rel="prev" href="../../../02/01/overfit10-deepseek-r1/">
      
      
        <link rel="next" href="../../../08/19/overfit12-dinov3/">
      
      
      <link rel="icon" href="../../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.18">
    
    
      
        <title>Overfit#11: V-JEPA 2 - Louis Stefanuto</title>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/stylesheets/main.7e37652d.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#overfit11-v-jepa-2" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../../.." title="Louis Stefanuto" class="md-header__button md-logo" aria-label="Louis Stefanuto" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Louis Stefanuto
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Overfit#11: V-JEPA 2
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
    
      <div class="md-header__source">
        <a href="https://github.com/LouisStefanuto" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    Github
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../about/" class="md-tabs__link">
        
  
  
    
  
  About

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../../../" class="md-tabs__link">
          
  
  
  Posts

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../../.." title="Louis Stefanuto" class="md-nav__button md-logo" aria-label="Louis Stefanuto" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Louis Stefanuto
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/LouisStefanuto" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    Github
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../about/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    About
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Posts
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Posts
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      <a href="../../../../" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Index
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Post series
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            Post series
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/alphafold/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AlphaFold
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/chemistry/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Chemistry
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/computer-vision/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Computer Vision
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/diffusion-models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Diffusion models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/drug-discovery/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Drug Discovery
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/embedding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Embedding
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/llm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LLM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/reinforcement-learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Reinforcement Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/robotics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Robotics
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/ssl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SSL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/video/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Video
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#motivation" class="md-nav__link">
    <span class="md-ellipsis">
      Motivation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jepa-in-a-nutshell" class="md-nav__link">
    <span class="md-ellipsis">
      JEPA in a nutshell
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#v-jepa-2" class="md-nav__link">
    <span class="md-ellipsis">
      V-JEPA 2
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#downstream-applications" class="md-nav__link">
    <span class="md-ellipsis">
      Downstream applications
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#v-jepa-2-ac-robotic-control" class="md-nav__link">
    <span class="md-ellipsis">
      V-JEPA 2-AC: Robotic control
    </span>
  </a>
  
    <nav class="md-nav" aria-label="V-JEPA 2-AC: Robotic control">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#inference" class="md-nav__link">
    <span class="md-ellipsis">
      Inference
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training" class="md-nav__link">
    <span class="md-ellipsis">
      Training
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      References
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
  <div class="md-content md-content--post" data-md-component="content">
    <div class="md-sidebar md-sidebar--post" data-md-component="sidebar" data-md-type="navigation">
      <div class="md-sidebar__scrollwrap">
        <div class="md-sidebar__inner md-post">
          <nav class="md-nav md-nav--primary">
            <div class="md-post__back">
              <div class="md-nav__title md-nav__container">
                <a href="../../../../" class="md-nav__link">
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
                  <span class="md-ellipsis">
                    Back to index
                  </span>
                </a>
              </div>
            </div>
            
              <div class="md-post__authors md-typeset">
                
                  <div class="md-profile md-post__profile">
                    <span class="md-author md-author--long">
                      <img src="https://github.com/LouisStefanuto.png" alt="Louis Stefanuto">
                    </span>
                    <span class="md-profile__description">
                      <strong>
                        
                          Louis Stefanuto
                        
                      </strong>
                      <br>
                      Creator
                    </span>
                  </div>
                
              </div>
            
            <ul class="md-post__meta md-nav__list">
              <li class="md-nav__item md-nav__item--section">
                <div class="md-post__title">
                  <span class="md-ellipsis">
                    Metadata
                  </span>
                </div>
                <nav class="md-nav">
                  <ul class="md-nav__list">
                    <li class="md-nav__item">
                      <div class="md-nav__link">
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 19H5V8h14m-3-7v2H8V1H6v2H5c-1.11 0-2 .89-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2h-1V1m-1 11h-5v5h5z"/></svg>
                        <time datetime="2025-06-25 00:00:00+00:00" class="md-ellipsis">June 25, 2025</time>
                      </div>
                    </li>
                    
                    
                      <li class="md-nav__item">
                        <div class="md-nav__link">
                          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9 3v15h3V3zm3 2 4 13 3-1-4-13zM5 5v13h3V5zM3 19v2h18v-2z"/></svg>
                          <span class="md-ellipsis">
                            in
                            
                              <a href="../../../../category/ssl/">SSL</a>, 
                              <a href="../../../../category/computer-vision/">Computer Vision</a>, 
                              <a href="../../../../category/embedding/">Embedding</a>, 
                              <a href="../../../../category/video/">Video</a>, 
                              <a href="../../../../category/robotics/">Robotics</a></span>
                        </div>
                      </li>
                    
                    
                      
                      <li class="md-nav__item">
                        <div class="md-nav__link">
                          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 20a8 8 0 0 0 8-8 8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8m0-18a10 10 0 0 1 10 10 10 10 0 0 1-10 10C6.47 22 2 17.5 2 12A10 10 0 0 1 12 2m.5 5v5.25l4.5 2.67-.75 1.23L11 13V7z"/></svg>
                          <span class="md-ellipsis">
                            
                              11 min read
                            
                          </span>
                        </div>
                      </li>
                    
                  </ul>
                </nav>
              </li>
            </ul>
            
          </nav>
          
        </div>
      </div>
    </div>
    <article class="md-content__inner md-typeset">
      
        
  



  
  


<h1 id="overfit11-v-jepa-2"><strong>Overfit#11:</strong> V-JEPA 2</h1>
<p><a href="./"><img alt="v jepa 2" src="../../../../images/11/main.jpg" /></a></p>
<!-- more -->

<h2 id="motivation">Motivation</h2>
<p>Yann Lecun claims that LLMs are a dead end on the path towards AGI. According to his claims, we need new architectures to better mimic the way we think, learn, understand, plan.</p>
<p>In 2022, he wrote a position paper in which he introduces his vision of what a smarter AI could look like. <strong>JEPA</strong><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup> - Joint Embedding Predictive Architecture - is his attempt in that direction. Developed at Meta FAIR, JEPA is a family of non-generative models, trained in a slightly different SSL manner, to learn world understanding and planning.</p>
<p>Two weeks ago (june 2025), Meta released a new model in the JEPA family: <strong>V-JEPA 2</strong>, which is the focus of this post.</p>
<div class="admonition quote">
<p class="admonition-title">Why did this new JEPA model catch my attention?</p>
</div>
<p>We have been hearing of JEPA models for two years now. I never took time to deep dive the papers, because it seemed still early-stage. I wanted to wait a bit, for the JEPA ecosystem to mature, and see how the research community reacted to these fresh ideas ...</p>
<p>V-JEPA 2 caught my attention because <strong>it is the first JEPA model with real-world applications</strong>. Image-JEPA and V-JEPA were mostly experiment papers, to see how far they could go with this SSL approach. V-JEPA 2 is the continuation of these papers and showcase super cool applications like <strong>conditioning an LLM</strong> for video Question Answering or zero-shot <strong>robot control</strong>.</p>
<p>Finally, pretraining models on large amounts of text or images showcased interesting emerging properties. I was curious to study what properties emerged when a new temporal dimension is added into the mix. Time is in fact needed to learn concepts like motion, gravity, planning ... Useful concepts and properties for a world model.</p>
<div class="admonition note">
<p class="admonition-title">Revolutionary approach, or simply a new flavor of the good old auto-encoders? Let's find out! ðŸ‘‡</p>
</div>
<figure>
<video controls="controls" src="https://video-cdg4-1.xx.fbcdn.net/o1/v/t2/f2/m69/AQN6DgLP8TuI-7z3giAEpJf-A15y2zeGoT3g6jsa3ibA6vZlEnUEfNJWpkGjGX9qQojwIIMABDmZpTdFVd9-zH1g.mp4?strext=1&amp;_nc_cat=104&amp;_nc_sid=5e9851&amp;_nc_ht=video-cdg4-1.xx.fbcdn.net&amp;_nc_ohc=BgCfX9EVQecQ7kNvwHH2H8q&amp;efg=eyJ2ZW5jb2RlX3RhZyI6Inhwdl9wcm9ncmVzc2l2ZS5GQUNFQk9PSy4uQzMuMTI4MC5kYXNoX2gyNjQtYmFzaWMtZ2VuMl83MjBwIiwieHB2X2Fzc2V0X2lkIjo0MTM0NzIyNjIzNDc5NjI4LCJ2aV91c2VjYXNlX2lkIjoxMDYyNiwiZHVyYXRpb25fcyI6MTUsInVybGdlbl9zb3VyY2UiOiJ3d3cifQ%3D%3D&amp;ccb=17-1&amp;vs=b4f65e52e218632a&amp;_nc_vs=HBkcFQIYOnBhc3N0aHJvdWdoX2V2ZXJzdG9yZS9HRDJHRng3dW9pMWdaZVFCQUp6Tnp1dEEzNHdXYnY0R0FBQUYVAALIARIAKAAYABsCiAd1c2Vfb2lsATEScHJvZ3Jlc3NpdmVfcmVjaXBlATEVAAAmmP28k8Sg2A4VAigCQzMsF0AuAAAAAAAAGBlkYXNoX2gyNjQtYmFzaWMtZ2VuMl83MjBwEQB1AmWEpgEA&amp;_nc_zt=28&amp;oh=00_AfR5F_Bb1KevslW_UUZ1I7306w5BGCmjVdDaWaVaulyk5Q&amp;oe=6882E51A" title="VJEPA2"></video>
<figcaption>Video from Meta AI blog post.</figcaption>
</figure>
<div class="admonition success">
<p class="admonition-title">Recommended lectures</p>
<p>Plenty of super talented writers already covered JEPA in depth. Their content is top-notch. If you have some time, I highly recommend reading these first, and then come back to this post:</p>
<ul>
<li><a href="https://www.turingpost.com/p/jepa">What is JEPA?</a> by the Turing Post</li>
<li><a href="https://rohitbandaru.github.io/blog/JEPA-Deep-Dive/">Deep Dive into Yann LeCunâ€™s JEPA</a> by Rohit Bandaru</li>
</ul>
<p>If your time is limited, no worries. Let me give you a dense summary of JEPA, Image-JEPA and Video-JEPA.</p>
</div>
<hr />
<h2 id="jepa-in-a-nutshell">JEPA in a nutshell</h2>
<blockquote>
<p>ðŸ‘‰ To keep things simple, I will explain JEPA by introducing I-JEPA, its image application. We will generalize back to JEPA and other modalities later.</p>
</blockquote>
<div class="admonition note">
<p class="admonition-title">What are the limitations of Large Image Models?</p>
</div>
<p>Large Image models are mostly trained in the signal space (pixels), leading them to pay attention to irrelevant details during learning. For instance, most image embedding models are trained to reconstruct masked or blurred images (MAEs, iBOT). Yet, asking a model to reconstruct a pixel-perfect image is an impossible task, because some details are almost random (the reflections of light on water, the position of bits of grass). Instead we would prefer them to focus on learning meaningful <strong>higher level representations</strong> (embeddings/concepts).</p>
<p>Moreover, LLMs have no <strong>planning capabilities</strong>. Given an observation of an environment (an image), they are poor predictors of what is coming next.</p>
<!-- And finally, most methods have a single model to encode images and predict the missing parts. This is a weakness. In fact, the model tries to reconstruct based on the information it has at its disposal. Sometimes information is missing, so it has no other choice than to make an assumption on the future, by choosing one of the possible continuation of the image. Take the example of a car reaching an intersection, turning left and right are both plausible continuations, and you can't guess which direction the car will take. To draw the continuation, you would have to make a choice. As a consequence, embeddings models tend to entangle these multiple possible futures when generating embeddings. We would rather prefer to split the responsibilities: one model for encoding what is known, and one model for predicting what is missing.

Doing so, we could first embed the known situation (the past), then sample possible actions (e.g. turning left/right) and then condition the second model to predict the embedding of the future based on the past + the sampled action. -->

<div class="admonition note">
<p class="admonition-title">What is I-JEPA's solution?</p>
</div>
<p>To build a powerful world model capable of abstraction, I-JEPA<sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup> is trained in the latent space. If you are familiar with self-distillation techniques like DINOv2 (I wrote a <a href="../../../01/13/overfit9-dinov2-explained-learning-robust-visual-features-without-supervision/"><strong>dedicated post on it, check it out</strong> ðŸŒŸ</a>), the model basically <strong>learns to predict the embeddings of the masked tokens of an image</strong>.</p>
<div class="admonition quote">
<p><figure markdown>
<img alt="bench" src="../../../../images/11/jepa.png" width="700" />
<figcaption>Image from Meta AI blog post.</figcaption>
</figure></p>
</div>
<p>I-JEPA is based on the the Vision Transformer (ViT) architecture and therefore processes images as sequences of patches. The model is made of three ViT submodules:</p>
<ul>
<li><strong>a context encoder</strong> <span class="arithmatex">\(f_\theta\)</span>: The masked image <span class="arithmatex">\(x\)</span>, also called the context, is encoded using a context ViT encoder. The outputs are <span class="arithmatex">\(s_x\)</span>.</li>
<li>
<p><strong>a target encoder</strong> <span class="arithmatex">\(f_\theta\)</span>: The original image <span class="arithmatex">\(y\)</span> (the bottom dog) is encoded using a target ViT encoder. The outputs are <span class="arithmatex">\(s_y\)</span>.</p>
<blockquote>
<p>Usually, the target encoder is an EMA of the context encoder. Like in DINO. This stabilizes training and avoids collapse.</p>
</blockquote>
</li>
<li>
<p><strong>a predictor</strong> <span class="arithmatex">\(g_\phi\)</span> that predicts the embeddings of the masked tokens of the image. Its outputs are <span class="arithmatex">\(\hat{s_y}\)</span>.</p>
</li>
</ul>
<div class="admonition bug">
<p>The regression loss is computed over the embeddings <span class="arithmatex">\(\mathcal{L}(s_y, \hat{s_y})\)</span>.</p>
</div>
<hr />
<p>As you can see the model respects the requirements we listed above in introduction:</p>
<ul>
<li>The models learn <strong>abstract concepts</strong>: they learn to reconstruct embeddings, not images. This encourages learning abstractions rather than focusing on details.</li>
<li>The encoding logic (encoder) is <strong>separated</strong> from the reconstruction logic (predictor). The encoder focuses on compressing information of what is known, while the predictor's goal is to infer what is missing.</li>
</ul>
<blockquote>
<p>For the moment, why the predictor is useful is maybe not straightforward, we'll see in Video-JEPA why the decoupling encoder/predictor is powerful for planning.</p>
</blockquote>
<p>I-JEPA showcased impressive performance on representation learning image benchmarks. It outperforms iBOT and MAE (two famous training techniques), for a training budget around one order of magnitude smaller. This improved sample efficiency is probably due to the fact that <strong>I-JEPA does not try to reconstruct pixel-perfect images</strong>, but instead focuses on predicting high-level <strong>semantic embeddings</strong>. By avoiding the need to capture every visual detail (like reflections or textures), the model can dedicate its capacity to learning meaningful abstractions, which accelerates training and improves generalization.</p>
<div class="admonition quote">
<p><figure markdown>
<img alt="bench" src="../../../../images/11/ijepa-perf.png" width="400" />
<figcaption>Image from I-JEPA paper [2].</figcaption>
</figure></p>
</div>
<p>As we saw in previous posts, <strong>evaluating an embedding model is ambiguous</strong>. The simplest way to validate the quality of the embeddings is usually to evaluate the performance of simple classification models (like a logistic regression) directly trained on its embeddings.</p>
<p>For V-JEPA 2, the authors also wanted to check if the features learned were just abstract, or if they still contained enough information to recover the masked pixels. Thus, they <strong>trained a decoder model, to reconstruct the masked pixels</strong>, from the latent embeddings. They observed that the trained decoder (only for evaluation, right) was able to reconstruct the images, even if the embeddings were never trained to perform a reconstruction task. Of course the decoder has poor reconstruction metrics compared to models pretrained to reconstruct, but this showcases that the features learnt are in fact meaningful.</p>
<div class="admonition quote">
<p><figure markdown>
<img alt="vjepa" src="../../../../images/11/ijepa-eval.png" width="400" />
<figcaption>Evaluation examples. Generations from the embeddings of I-JEPA.<br>Image from I-JEPA paper [4].</figcaption>
</figure></p>
</div>
<div class="admonition success">
<p class="admonition-title">Generalization to JEPA</p>
<p>As we saw, the I-JEPA is specific to images. But it can easily be generalized to any type of inputs, as long as they can be splitted into (masked) tokens.</p>
<ul>
<li>Take an input sequence.</li>
<li>Mask some tokens.</li>
<li>Encode both the masked and unmasked sequences.</li>
<li>Predict the embeddings of the masked tokens using the predictor. The predictor also gets the mask as input condition for guidance.</li>
<li>The loss is simply computed over the embeddings.</li>
</ul>
<p><figure markdown>
<img alt="bench" src="../../../../images/11/jepa-paper-diagram.png" width="600" />
<figcaption>Image Credit: Lecun's position paper.</figcaption>
</figure></p>
</div>
<h2 id="v-jepa-2">V-JEPA 2</h2>
<blockquote>
<p>Now that you have the fundamentals of JEPA, let's jump directly to V-JEPA 2, which is basically the same as V-JEPA 1<sup id="fnref:3"><a class="footnote-ref" href="#fn:3">3</a></sup> but with more data and bigger ViT models.</p>
</blockquote>
<p>V-JEPA 2<sup id="fnref:4"><a class="footnote-ref" href="#fn:4">4</a></sup> is a billion-parameter model, pretrained on over a million hours of <strong>video data</strong>, using the JEPA's self-supervized training procedure.</p>
<p>Like its little brother I-JEPA, the video frames are patchified. Let <span class="arithmatex">\(N\)</span> the number of frames in a video. The video tensor <span class="arithmatex">\((N, H, W)\)</span> is split into a sequence of <span class="arithmatex">\(L\)</span> tokens called <strong>tubelets</strong> where each tubelet is of shape <span class="arithmatex">\((2, 16, 16)\)</span>.</p>
<blockquote>
<p>The 2 means that each token is made of two consecutive frames. As usual in transformers, they incorporate the position of the patches through (3D ROPE) positional encodings.</p>
</blockquote>
<div class="admonition quote">
<p><figure markdown>
<img alt="vjepa" src="../../../../images/11/vjepa2.png" width="400" />
<figcaption>Image from V-JEPA 2 paper [4].</figcaption>
</figure></p>
</div>
<p>The training procedure is similar to I-JEPA. The predictor is trained to recover the embeddings of masked tubelets, from the embeddings of the unmasked tubelets. The teacher model is an EMA of the student, the stop-gradient operation blocks the gradient flow to the teacher. A very high masking ratio is used, with around 90% of the pixels masked. This high masking ratio may be due to the fact that information in video is redundant, so you need/can mask much more to reduce leakage and force the model to learn rather than copying.</p>
<p>After training, we get a strong video patch embedder (encoder) and a versatile patch predictor.</p>
<div class="admonition quote">
<p class="admonition-title">More about the paper ...</p>
<p>ðŸ‘¾ Github: <a href="https://github.com/facebookresearch/vjepa2">https://github.com/facebookresearch/vjepa2</a></p>
<p>ðŸ“š Arxiv: <a href="https://arxiv.org/abs/2506.09985">https://arxiv.org/abs/2506.09985</a></p>
</div>
<h2 id="downstream-applications">Downstream applications</h2>
<p><strong>After SSL pre-training, V-JEPA 2 is essentially an embedding model</strong>. It is not useful out-of-the-box ... but it can easily become after post training.</p>
<p>The paper showcases multiple applications of V-JEPA 2. In fact (see diagram below), after a task-specific post-training (in green), one can evaluate its world Understanding, Prediction or Planning capabilities.</p>
<p>I will focus on the robotic application that I find super impressive. Check out the paper if you are interested by the other applications.</p>
<div class="admonition quote">
<p><figure markdown>
<img alt="vjepa" src="../../../../images/11/vjepa2-data.png" width="600" />
<figcaption>Image from V-JEPA 2 paper [4].</figcaption>
</figure></p>
</div>
<h2 id="v-jepa-2-ac-robotic-control">V-JEPA 2-AC: Robotic control</h2>
<p>This experiment focuses on <strong>zero-shot robotic planning for pick-and-place tasks using only image goals</strong>. In simpler terms: you show the robot a picture of the goal (e.g. a ball inside a cup), and the model figures out by itself how to reach that state, <strong>without any task-specific training, supervision, or reward function</strong>.</p>
<figure>
<video controls="controls" src="https://video-cdg4-1.xx.fbcdn.net/o1/v/t2/f2/m69/AQNACsVL3qCe48eGGe6HHokPD5LLldZ6rGFUCjmHg2ASRV4IT9aowjqr0ruYcUeUA4J_fOypR0Hhw1hcMZWJYjIS.mp4?strext=1&amp;_nc_cat=102&amp;_nc_sid=5e9851&amp;_nc_ht=video-cdg4-1.xx.fbcdn.net&amp;_nc_ohc=Xob21nQNmbcQ7kNvwEaf35p&amp;efg=eyJ2ZW5jb2RlX3RhZyI6Inhwdl9wcm9ncmVzc2l2ZS5GQUNFQk9PSy4uQzMuMTkyMC5kYXNoX2gyNjQtYmFzaWMtZ2VuMl8xMDgwcCIsInhwdl9hc3NldF9pZCI6MTMzOTUzNjE5NDE5ODU0MCwidmlfdXNlY2FzZV9pZCI6MTA2MjYsImR1cmF0aW9uX3MiOjM0LCJ1cmxnZW5fc291cmNlIjoid3d3In0%3D&amp;ccb=17-1&amp;vs=189d57d89cbbec9f&amp;_nc_vs=HBksFQIYOnBhc3N0aHJvdWdoX2V2ZXJzdG9yZS9HTW56Qmg2NGlzS1dVMlFFQUppQ3E4T1lvT2dOYnY0R0FBQUYVAALIARIAFQIYOnBhc3N0aHJvdWdoX2V2ZXJzdG9yZS9HTzlWRWg1SV9GY2JGOW9FQVA2ZlpyUVNlVnBYYnY0R0FBQUYVAgLIARIAKAAYABsCiAd1c2Vfb2lsATEScHJvZ3Jlc3NpdmVfcmVjaXBlATEVAAAmmPCmsKKT4QQVAigCQzMsF0BBTvnbItDlGBpkYXNoX2gyNjQtYmFzaWMtZ2VuMl8xMDgwcBEAdQJlhKYBAA&amp;_nc_zt=28&amp;oh=00_AfT8x5fYqHSJhmnQ66rKI2TenMddqKJzQQJCo2g5suOTmw&amp;oe=6882DC64" title="Robot arm control"></video>
<figcaption>Video from Meta AI blog post.</figcaption>
</figure>
<blockquote>
<p>Let's first see how we could use V-JEPA 2 to plan and control the robot. We'll then see how to post-train it to achieve our goals.</p>
</blockquote>
<h3 id="inference">Inference</h3>
<p>Letâ€™s say we are at time <span class="arithmatex">\(t\)</span> with a current image <span class="arithmatex">\(x_t\)</span> (e.g. the ball next to the cup), and we want the scene to become like <span class="arithmatex">\(x_{\text{target}}\)</span> (e.g. the ball inside the cup).</p>
<div class="admonition quote">
<ol>
<li>
<p><strong>Encode the current and target frames</strong> using the V-JEPA 2 encoder:</p>
<ul>
<li><span class="arithmatex">\(s_t = f_\theta(x_t)\)</span>  </li>
<li><span class="arithmatex">\(s_{\text{target}} = f_\theta(x_{\text{target}})\)</span></li>
</ul>
</li>
<li>
<p><strong>Sample a batch of candidate actions</strong> (e.g. 100 random action trajectories like joint positions or movements).</p>
</li>
<li>
<p>For each action <span class="arithmatex">\(a_k\)</span>, use the <strong>predictor</strong> to simulate the future embedding:</p>
<ul>
<li><span class="arithmatex">\(\hat{s}_{t+1}^{(k)} = g_\phi(s_t, a_k)\)</span></li>
</ul>
</li>
<li>
<p><strong>Compute the distance</strong> between the predicted embedding and the target one:</p>
<ul>
<li><span class="arithmatex">\(d_k = \| \hat{s}_{t+1}^{(k)} - s_{\text{target}} \|\)</span></li>
</ul>
</li>
<li>
<p><strong>Choose the trajectory</strong> that minimizes this distance in N steps.</p>
</li>
<li>
<p>Execute the first action of the best trajectory  <span class="arithmatex">\(a^*\)</span>, observe the new frame, and repeat the process until convergence.</p>
</li>
</ol>
<blockquote>
<p>In practice, they sampled actions using the Cross-Entropy Method (CEM). At each step, a distribution over actions is updated to concentrate on those that best match the target state. Only the first action of the best trajectory is executed before re-planning.</p>
</blockquote>
</div>
<div class="admonition quote">
<p><figure markdown>
<img alt="vjepa" src="../../../../images/11/vjepa2ac-conditions.png" width="600" />
<figcaption>The goal is given via a target image. That's all!<br>Image from V-JEPA 2 paper [4].</figcaption>
</figure></p>
</div>
<div class="admonition success">
<p>Whatâ€™s particularly interesting here is that <strong>we donâ€™t predict actions directly</strong>. Instead, we <strong>sample candidate actions</strong> and use the <strong>reconstruction loss as a proxy for energy</strong> to evaluate them. The idea is simple but powerful: the better an action helps the model predict a future state close to the target, the lower its energy. This allows a <strong>non-generative model to be used for planning</strong>, by selecting actions that minimize prediction errorâ€”without ever explicitly generating the future.</p>
</div>
<h3 id="training">Training</h3>
<div class="admonition note">
<p class="admonition-title">V-JEPA 2, as pretrained, is not immediately usable for robot control. Why?</p>
</div>
<p>Because during its SSL pretraining, the predictor <strong>never learns the causal impact of actions</strong>. It only learns to predict the future frames of videos, not how actions modify the world.</p>
<p>To adapt the frozen V-JEPA 2 encoder for robotic control, the authors retrain the <strong>predictor</strong> to learn how actions modify the visual world. This predictor takes in a temporally interleaved sequence of (action, state, frame embedding) tuples, where frame embeddings are extracted via the frozen encoder, and the state/action information comes from robot proprioception. The model is trained to predict the embedding of the next frame from the current (action, state, frame), and minimizes an <span class="arithmatex">\(\ell_1\)</span> loss between the predicted and ground-truth embeddings.</p>
<div class="admonition quote">
<p class="admonition-title">In other words:</p>
<ul>
<li>They <strong>freeze the encoder</strong> (trained on 1M hours of video).</li>
<li>Then, they <strong>post-train the predictor</strong> on 62 hours of robot interaction data (from the DROID dataset), where each frame is paired with the robot's joint velocities and states.</li>
<li>The updated predictor now takes as input:<ul>
<li>an encoded state,</li>
<li>and a candidate action,</li>
<li>and predicts the embedding of the next frame.</li>
</ul>
</li>
</ul>
</div>
<div class="admonition quote">
<p><figure markdown>
<img alt="vjepa" src="../../../../images/11/vjepa2ac.png" width="600" />
<figcaption>Image from V-JEPA 2 paper [4].</figcaption>
</figure></p>
</div>
<p>In addition to predicting the next step, the predictor is trained with a <strong>rollout loss</strong> to simulate short multi-step futures, mimicking planning scenarios. Starting from an initial state and frame embedding, the predictor autoregressively simulates two steps ahead using a sampled sequence of actions. The predicted final embedding is compared to the ground truth embedding at <span class="arithmatex">\(T+1\)</span>, again using an <span class="arithmatex">\(\ell_1\)</span> loss. This dual objective â€” <strong>teacher forcing + rollout loss</strong> â€” ensures the model not only predicts accurately step-by-step, but also remains stable and consistent over multiple planning steps.</p>
<div class="admonition success">
<p class="admonition-title">What makes this application stand out</p>
<blockquote>
<p>You end up with a <strong>zero-shot robotic planning system</strong> capable of solving pick-and-place tasks from image goals â€” without requiring dense rewards, task-specific demonstrations, or hand-labeled annotations.</p>
</blockquote>
<p>The only requirements are:</p>
<ul>
<li>a pretrained V-JEPA 2 encoder (frozen),</li>
<li>a small dataset of robot interactions (videos + action labels),</li>
<li>and some clever inference using CEM.</li>
</ul>
<p>Thatâ€™s <strong>SSL with real-world impact</strong>, and I find that pretty exciting.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Limitations</p>
<p>This SSL approach doesn't generalize to all robotic arms (yet?). It still requires to collect videos and robot positions for the particular robot you want to control.</p>
<p>This is cheaper than building a fully annotated dataset, but that remains a big limitation.</p>
</div>
<hr />
<div class="admonition note">
<p class="admonition-title">Concluding remarks</p>
<p>JEPA is an interesting approach that brings a bit of fresh air in the brute-force Transformer/Next-Token-Prediction wave. On my side, I am especially curious to see how the JEPA architectures will evolve over time to embrace the whole position paper of Yann Lecun. In fact, the current architectures only address understanding and planning. What about the configurator? ... (once again, I warmly recommend to read the deep dives quoted above).</p>
<p>I hope you enjoyed reading this technical deep dive. If so, <strong>feel free to share it and connect</strong> ðŸ˜Š</p>
</div>
<h2 id="references">References</h2>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p><strong>JEPA Paper</strong>: <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf">A Path Towards Autonomous Machine Intelligence</a>. Version 0.9.2, 2022-06-27&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p><strong>I-JEPA Paper</strong>: Assran, M., Duval, Q., Misra, I., Bojanowski, P., Vincent, P., Rabbat, M., ... &amp; Ballas, N. (2023). <a href="https://arxiv.org/abs/2301.08243">Self-supervised learning from images with a joint-embedding predictive architecture</a>. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 15619-15629).&#160;<a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:3">
<p><strong>V-JEPA Paper</strong>: Bardes, A., Garrido, Q., Ponce, J., Chen, X., Rabbat, M., LeCun, Y., ... &amp; Ballas, N. (2024). <a href="https://arxiv.org/abs/2404.08471">Revisiting feature prediction for learning visual representations from video</a>. arXiv preprint arXiv:2404.08471.&#160;<a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:4">
<p><strong>V-JEPA 2 Paper</strong>: Assran, M., Bardes, A., Fan, D., Garrido, Q., Howes, R., Muckley, M., ... &amp; Ballas, N. (2025). <a href="https://arxiv.org/abs/2506.09985">V-JEPA 2: Self-Supervised Video Models Enable Understanding</a>, Prediction and Planning. arXiv preprint arXiv:2506.09985.&#160;<a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
</ol>
</div>







  
  




  



      
    </article>
  </div>

          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/LouisStefanuto" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.linkedin.com/in/louis-stefanuto/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../../../..", "features": ["navigation.tabs", "navigation.expand"], "search": "../../../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../../../assets/javascripts/bundle.92b07e13.min.js"></script>
      
        <script src="../../../../../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>