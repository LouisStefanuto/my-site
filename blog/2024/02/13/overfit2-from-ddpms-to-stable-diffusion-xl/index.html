
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Personal website of Louis Stefanuto">
      
      
      
      
        <link rel="prev" href="../../01/overfit1-denoising-diffusion-probabilistic-models---basics-of-diffusion-models/">
      
      
        <link rel="next" href="../../24/overfit3-how-does-video-generation-work---insights-into-stable-video-diffusion/">
      
      
      <link rel="icon" href="../../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.18">
    
    
      
        <title>Overfit#2: From DDPMs to Stable Diffusion XL - Louis Stefanuto</title>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/stylesheets/main.7e37652d.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#overfit2-from-ddpms-to-stable-diffusion-xl" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../../.." title="Louis Stefanuto" class="md-header__button md-logo" aria-label="Louis Stefanuto" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Louis Stefanuto
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Overfit#2: From DDPMs to Stable Diffusion XL
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
    
      <div class="md-header__source">
        <a href="https://github.com/LouisStefanuto" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    Github
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../about/" class="md-tabs__link">
        
  
  
    
  
  About

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../../../" class="md-tabs__link">
          
  
  
  Posts

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../../.." title="Louis Stefanuto" class="md-nav__button md-logo" aria-label="Louis Stefanuto" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Louis Stefanuto
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/LouisStefanuto" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    Github
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../about/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    About
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Posts
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Posts
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      <a href="../../../../" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Index
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Post series
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            Post series
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/alphafold/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AlphaFold
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/chemistry/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Chemistry
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/computer-vision/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Computer Vision
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/diffusion-models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Diffusion models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/drug-discovery/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Drug Discovery
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/embedding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Embedding
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/llm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LLM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/reinforcement-learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Reinforcement Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/robotics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Robotics
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/ssl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SSL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/video/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Video
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#motivation" class="md-nav__link">
    <span class="md-ellipsis">
      Motivation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#improved-ddpms-openai" class="md-nav__link">
    <span class="md-ellipsis">
      Improved DDPMs (OpenAI)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Improved DDPMs (OpenAI)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cosine-noise-scheduling" class="md-nav__link">
    <span class="md-ellipsis">
      Cosine noise scheduling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-variances" class="md-nav__link">
    <span class="md-ellipsis">
      Learning variances
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#importance-sampling" class="md-nav__link">
    <span class="md-ellipsis">
      Importance sampling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#results" class="md-nav__link">
    <span class="md-ellipsis">
      Results
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#stable-diffusion" class="md-nav__link">
    <span class="md-ellipsis">
      Stable Diffusion
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Stable Diffusion">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#latent-diffusion-models" class="md-nav__link">
    <span class="md-ellipsis">
      Latent diffusion models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#results_1" class="md-nav__link">
    <span class="md-ellipsis">
      Results
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sdxl" class="md-nav__link">
    <span class="md-ellipsis">
      SDXL
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SDXL">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bigger-models" class="md-nav__link">
    <span class="md-ellipsis">
      Bigger models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#micro-conditioning" class="md-nav__link">
    <span class="md-ellipsis">
      Micro-conditioning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#refiner" class="md-nav__link">
    <span class="md-ellipsis">
      Refiner
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#additional-ideas" class="md-nav__link">
    <span class="md-ellipsis">
      Additional ideas
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#results-and-limitations" class="md-nav__link">
    <span class="md-ellipsis">
      Results and limitations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#code-example" class="md-nav__link">
    <span class="md-ellipsis">
      Code example
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    <span class="md-ellipsis">
      Conclusion
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
  <div class="md-content md-content--post" data-md-component="content">
    <div class="md-sidebar md-sidebar--post" data-md-component="sidebar" data-md-type="navigation">
      <div class="md-sidebar__scrollwrap">
        <div class="md-sidebar__inner md-post">
          <nav class="md-nav md-nav--primary">
            <div class="md-post__back">
              <div class="md-nav__title md-nav__container">
                <a href="../../../../" class="md-nav__link">
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
                  <span class="md-ellipsis">
                    Back to index
                  </span>
                </a>
              </div>
            </div>
            
              <div class="md-post__authors md-typeset">
                
                  <div class="md-profile md-post__profile">
                    <span class="md-author md-author--long">
                      <img src="https://github.com/LouisStefanuto.png" alt="Louis Stefanuto">
                    </span>
                    <span class="md-profile__description">
                      <strong>
                        
                          Louis Stefanuto
                        
                      </strong>
                      <br>
                      Creator
                    </span>
                  </div>
                
              </div>
            
            <ul class="md-post__meta md-nav__list">
              <li class="md-nav__item md-nav__item--section">
                <div class="md-post__title">
                  <span class="md-ellipsis">
                    Metadata
                  </span>
                </div>
                <nav class="md-nav">
                  <ul class="md-nav__list">
                    <li class="md-nav__item">
                      <div class="md-nav__link">
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 19H5V8h14m-3-7v2H8V1H6v2H5c-1.11 0-2 .89-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2h-1V1m-1 11h-5v5h5z"/></svg>
                        <time datetime="2024-02-13 00:00:00+00:00" class="md-ellipsis">February 13, 2024</time>
                      </div>
                    </li>
                    
                    
                      <li class="md-nav__item">
                        <div class="md-nav__link">
                          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9 3v15h3V3zm3 2 4 13 3-1-4-13zM5 5v13h3V5zM3 19v2h18v-2z"/></svg>
                          <span class="md-ellipsis">
                            in
                            
                              <a href="../../../../category/diffusion-models/">Diffusion models</a></span>
                        </div>
                      </li>
                    
                    
                      
                      <li class="md-nav__item">
                        <div class="md-nav__link">
                          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 20a8 8 0 0 0 8-8 8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8m0-18a10 10 0 0 1 10 10 10 10 0 0 1-10 10C6.47 22 2 17.5 2 12A10 10 0 0 1 12 2m.5 5v5.25l4.5 2.67-.75 1.23L11 13V7z"/></svg>
                          <span class="md-ellipsis">
                            
                              12 min read
                            
                          </span>
                        </div>
                      </li>
                    
                  </ul>
                </nav>
              </li>
            </ul>
            
          </nav>
          
        </div>
      </div>
    </div>
    <article class="md-content__inner md-typeset">
      
        
  



  
  


<h1 id="overfit2-from-ddpms-to-stable-diffusion-xl"><strong>Overfit#2:</strong> From DDPMs to Stable Diffusion XL</h1>
<p><img alt="Image main" src="../../../../images/2/main.jpg" /></p>
<!-- more -->

<p><em>From DDPMs to Stable Diffusion XL ...</em></p>
<h2 id="motivation">Motivation</h2>
<p>In the <strong><a href="../../01/overfit1-denoising-diffusion-probabilistic-models---basics-of-diffusion-models/">previous post</a></strong>, we explored DDPM, the foundational paper from <a href="https://arxiv.org/abs/2006.11239">Ho &amp; al. (2020)</a> that paved the way for Diffusion Models.</p>
<p>The DDPM paper was published back in 2020: lots of progress has been made since then. In this second post, we will deep dive into the improvements added since. In the end, we will uncover the key additional components of SDXL - <strong>Stable Diffusion XL</strong> - the most downloaded text-to-image architecture on Huggingface.</p>
<div class="admonition note">
<p class="admonition-title">Table of content</p>
<ul>
<li><a href="#improved-ddpms-openai">Improved DDPMs (OpenAI)</a></li>
<li><a href="#stable-diffusion">Stable Diffusion</a></li>
<li><a href="#sdxl">Stable Diffusion XL</a></li>
</ul>
</div>
<hr />
<h2 id="improved-ddpms-openai">Improved DDPMs (OpenAI)</h2>
<p>In February 2021 comes the first major upgrade to DDPMs ... <a href="https://arxiv.org/abs/2102.09672">"Improved DDPMs"</a> (a really creative name btw ...).</p>
<p>Published by OpenAI research scientists, this paper focused on squeezing extra performance from DDPMs through <strong>3 small improvements</strong>: changing the noise scheduler, learning the variances, and importance sampling.</p>
<figure>
<p><img alt="examples" src="../../../../images/2/examples.jpg" width="400" />
  </p>
<figcaption>Class-conditional ImageNet 64 Ã— 64 generated samples.</figcaption>
</figure>
<h3 id="cosine-noise-scheduling">Cosine noise scheduling</h3>
<div class="admonition quote">
<p class="admonition-title">Refresher from the <a href="../../01/overfit1-denoising-diffusion-probabilistic-models---basics-of-diffusion-models/">post about DDPMs</a></p>
<p><span class="arithmatex">\(\beta_t\)</span> = variance of noising step <span class="arithmatex">\(t-1 \rightarrow t\)</span></p>
<p><span class="arithmatex">\(\bar{\alpha_t}\)</span> = <strong>how much information the noised image still contains</strong> after <span class="arithmatex">\(0 \rightarrow 1 \rightarrow 2 \rightarrow \cdots \rightarrow t\)</span></p>
</div>
<p>DDPMs use a <strong>linear noise schedule</strong> for the forward diffusion process: the variances <span class="arithmatex">\(\beta_t\)</span> increase proportionally with the time step <span class="arithmatex">\(t\)</span> (from <span class="arithmatex">\(10^{-4}\)</span> to <span class="arithmatex">\(0.02\)</span>).</p>
<p><a href="https://arxiv.org/abs/2102.09672">Nichol &amp; Dhariwal (2021)</a> showed that (1) the image is actually destroyed too fast and that (2) the last noising steps don't do much. As you can see on the plot below, with a linear scheduler (blue), the information <span class="arithmatex">\(\bar{\alpha_t}\)</span> is in fact lost progressively, but mostly during the first steps (left), while the <span class="arithmatex">\(\bar{\alpha_t}\)</span> comes to a plateau on the last quarter of steps.</p>
<figure>
<p><img alt="noise scheduler" src="../../../../images/2/noise_schedulers.png" width="400" />
  </p>
<figcaption>Cosine noise scheduling enables achieving more linear alphas.<br>From <a href="https://arxiv.org/abs/2102.09672"> Improved DDPMs (2021)</a> </figcaption>
</figure>
<p>To mitigate these issues, <a href="https://arxiv.org/abs/2102.09672">Nichol &amp; Dhariwal (2021)</a> replaced the linear noise scheduling with a cosine noise scheduling (orange). The biggest advantage of this new noise scheduling is that (1) the image is much less modified during the first noising steps / last generation steps, thus helping the model in its refinement final steps, and (2) the image noising is almost linear during the intermediate steps. This led to better performance and helped make every denoising step useful.</p>
<figure>
<p><img alt="scheduler effect comparison" src="../../../../images/2/cosine.jpg" width="600" />
  </p>
<figcaption>Using cosine noise scheduling, we get a much smoother noising process. From <a href="https://arxiv.org/abs/2102.09672"> Improved DDPMs (2021)</a> </figcaption>
</figure>
<h3 id="learning-variances">Learning variances</h3>
<p>In DDPM, <a href="https://arxiv.org/abs/2006.11239">Ho. al (2020)</a> fixed the reverse process variances because learning them made the training unstable. Improved DDPM's authors claim it causes performance loss, as we don't optimize the exact loss function. Thus they make variances learnable. Yet, to mitigate the instability issues, they introduce a <strong>hybrid loss</strong>, made of the simplified DDPM loss + the exact loss <span class="arithmatex">\(L_{vlb}\)</span> that acts as a regularization term:</p>
<div class="arithmatex">\[
L_{hybrid} = L_{simplified} + \lambda L_{vlb} \quad \text{where } \lambda = 0.001
\]</div>
<figure>
<p><img alt="loss comparison" src="../../../../images/2/loss.jpg" width="600" />
  </p>
<figcaption>The hybrid loss (orange) is more stable and gives better results than the exact loss (blue). Using importance sampling further stabilizes the training. </figcaption>
</figure>
<p>Additionally, <a href="https://arxiv.org/abs/2102.09672">Nichol &amp; Dhariwal (2021)</a> found it was better to guess the variances <span class="arithmatex">\(\Sigma\)</span> using an interpolation trick. As <span class="arithmatex">\(\Sigma\)</span> is bounded by <span class="arithmatex">\(\beta_t\)</span> and <span class="arithmatex">\(\tilde{\beta_t}\)</span>, they trained the model to guess an interpolation vector <span class="arithmatex">\(v\)</span> between these two bounds:</p>
<div class="arithmatex">\[
\Sigma_\theta(\mathbf{x}_t, t) = \exp \left( v \log{\beta_t} + (1-v) \log{\tilde{\beta_t}} \right)
\]</div>
<h3 id="importance-sampling">Importance sampling</h3>
<p><a href="https://arxiv.org/abs/2102.09672">Nichol &amp; Dhariwal (2021)</a> analyzed on which steps the denoising model struggled the most. They showed that most of the loss is imputable to the first diffusion steps / last denoising steps. Thus they used <strong>importance sampling</strong> to mitigate this weakness.</p>
<p>The core idea of importance sampling is to show the model more samples on which it struggles. It is like preparing for an exam. In theory you should spend as much time on each chapter (uniform sampling). But in practice, you know that you perform poorly on some specific chapters, so you spend more time studying them (importance sampling).</p>
<p>At training time, <a href="https://arxiv.org/abs/2102.09672">Nichol &amp; Dhariwal (2021)</a> sampled <span class="arithmatex">\(t\)</span> values with a probability proportional to the average mean squared error you made on the 10 latest samples for this time step:</p>
<div class="arithmatex">\[
L_{vlb} = E_{t \sim p_t} \left[ \frac{L_t}{p_t}\right] \, \text{where } p_t \propto \sqrt{E[L_t^2]} \text{and } \sum p_t = 1
\]</div>
<p>Note that the authors mentioned that importance sampling helps when minimizing <span class="arithmatex">\(L_{vlb}\)</span> but not with <span class="arithmatex">\(L_{hybrid}\)</span>.</p>
<div class="grid cards">
<ul>
<li><figure markdown>
    <img alt="loss comparison" src="../../../../images/2/importance_sampling_1.png" width="600" />
    <figcaption>Importance sampling slightly changes the training procedure</figcaption>
  </figure></li>
<li><figure markdown>
    <img alt="loss comparison" src="../../../../images/2/importance_sampling_2.png" width="600" />
    <figcaption>The first noising steps / last denoising steps are accountable for most of the loss</figcaption>
  </figure></li>
</ul>
</div>
<h3 id="results">Results</h3>
<ul>
<li><strong>GANs competitors</strong>: With these small improvements, improved DDPMs are on par with GANs in terms of NLL (negative log-likelihood), while achieving much better mode coverage. They showed it in another paper <a href="https://arxiv.org/abs/2105.05233">"Diffusion Models Beat GANs on Image Synthesis" (2021)</a>.</li>
<li><strong>Improved DDPMs allow for skipping some inference steps</strong>: using 50 steps at inference with adapted variances instead of 4000 during training, they achieved a <span class="arithmatex">\(\times 80\)</span> faster inference while maintaining the same sample quality. Generating images is thus a matter of seconds instead of minutes which <strong>makes diffusion models applicable to products!</strong></li>
<li><strong>The scale potential:</strong> They showed there was still room for improvement by scaling up the size of the models.</li>
</ul>
<figure>
<p><img alt="scaling" src="../../../../images/2/scaling.png" width="500" />
  </p>
<figcaption>The bigger the models the better the performance. And back in 2021, the plateau has not been reached yet ...</figcaption>
</figure>
<hr />
<h2 id="stable-diffusion">Stable Diffusion</h2>
<p>The next major enhancement to DDPMs comes from a collaboration between Stability AI and RunWay: a paper entitled <a href="https://arxiv.org/abs/2112.10752">"High-Resolution Image Synthesis with Latent Diffusion Models"</a> published in December 2021.</p>
<p>Eight months later, in August 2022, the same authors trained a model on a subset of the <a href="https://laion.ai/blog/laion-5b/">LAION-5B</a> database and released <a href="https://github.com/CompVis/stable-diffusion"><strong>Stable Diffusion</strong></a>, an 860M UNet and 123M CLIP ViT-L/14 text encoder architecture.</p>
<p>Released under the permissive <a href="https://github.com/CompVis/stable-diffusion/blob/main/LICENSE">CreativeML OpenRAIL M license</a>, the model is relatively lightweight and runs on a GPU with at least 10GB VRAM. And it introduces a completely new concept: <strong>Latent Diffusion Models</strong>.</p>
<figure>
<p><img alt="" src="https://github.com/CompVis/stable-diffusion/blob/main/assets/stable-samples/txt2img/merged-0006.png?raw=true" />
  </p>
<figcaption>Images generated by Stable Diffusion v1<br>From the official <a href="https://github.com/CompVis/stable-diffusion.">Github repo</a></figcaption>
</figure>
<h3 id="latent-diffusion-models">Latent diffusion models</h3>
<p>Vanilla DDPMs denoise in the image space: they take an image as input, and they return a noise of the same dimension. I see two issues with it:</p>
<ul>
<li><strong>It is computationally expensive</strong>: the dimensions are huge (<span class="arithmatex">\(512 \times 512 \times 3\)</span> for instance), can we be more efficient?</li>
<li><strong>We work on pixel values</strong>: it is a fine-grained task, is it really what we want? Let's say I want to generate a cat, I don't care if every single pixel value is perfect, I just want the overall image to look like a realistic cat.</li>
</ul>
<div class="admonition quote">
<p class="admonition-title">What do Latent Diffusion Models do differently?</p>
</div>
<p><strong>Latent Diffusion Models</strong> (LDM) run the denoising process <strong>in a latent (embedding) space instead of the pixel space</strong>. Instead of denoising the image itself, the denoising is made on a compressed embedded version of the image.</p>
<p>Latent Diffusion Models (1) embed the image into a highly compressed feature map using an encoder <span class="arithmatex">\(\mathcal{E}\)</span>, (2) apply the denoising process on the embedding guided by a conditioning text/image/sequence, (3) decode the embedding to recover the image using a decoder <span class="arithmatex">\(\mathcal{D}\)</span>.</p>
<figure>
<p><img alt="Stable Diffusion architecture" src="https://ommer-lab.com/wp-content/uploads/2022/08/article-Figure3-1-1024x508.png" width="500" />
  </p>
<figcaption>Stable Diffusion architecture. Extended from <a href="https://arxiv.org/abs/2112.10752" title="Stable Diffusion paper"> Rombach, Blattmann &amp; al. (2022)</a> </figcaption>
</figure>
<div class="admonition quote">
<p class="admonition-title">WOH! That's an ugly architecture, what is going on here?</p>
</div>
<p>Let me split the aforementioned architecture to make things easier. The typical LDM architecture is made of 4 parts.</p>
<figure>
<p><img alt="encoder_decoder" src="../../../../images/2/splitted_ldm.jpg" />
  </p>
<figcaption>The 3 parts of a LDM. </figcaption>
</figure>
<ol>
<li><strong>Encoder</strong>: the authors used pre-trained regularized Variational Auto Encoders (VAE). They tested two flavors of VAEs, <a href="https://arxiv.org/abs/1711.00937">vq-VAEs</a> and KL-VAEs. Their purpose is simple: compress the image into a dense embedding <span class="arithmatex">\(z\)</span>, in a meaningful continuous embedding space (enforced by VAEs).</li>
<li><strong>DDPM</strong>: once the image is embedded, the whole denoising process is applied on <span class="arithmatex">\(z\)</span> instead of <span class="arithmatex">\(x\)</span> like in vanilla DDPMs. That is why we call them <strong>Latent</strong> diffusion models.</li>
<li><strong>Conditioning</strong>: as in DDPMs, you might want to condition the image generation with some additional input. For text prompts, a common choice is a CLIP model. The conditioning embedding is then infused in the denoising process using <a href="../../01/overfit1-denoising-diffusion-probabilistic-models---basics-of-diffusion-models/#cross-attention">cross-attention blocks</a>.</li>
<li><strong>Decoder</strong>: once denoised <span class="arithmatex">\(z\)</span> is converted back to the image space to recover an image <span class="arithmatex">\(\tilde{x}\)</span> using the decoder part <span class="arithmatex">\(\mathcal{D}\)</span> of the pre-trained VAE.</li>
</ol>
<p>An image is worth 1000 words, so here is a quick recap of the LDM's training and inference procedures:</p>
<figure>
<p><img alt="ldm" src="../../../../images/2/ldm.jpg" width="700" />
  </p>
<figcaption>At inference time, no need of the encoder and of the forward diffusion process. </figcaption>
</figure>
<h3 id="results_1">Results</h3>
<ul>
<li><strong>Better images</strong>: The model focuses on semantics, because the "high-frequency, imperceptible details are abstracted away".</li>
</ul>
<figure>
<p><img alt="metrics" src="../../../../images/2/metrics.png" width="500" />
  </p>
<figcaption>LDM-4 beat previous architectures on CelebA-HQ dataset and perform on par with GANs on other benchmarks. From <a href="https://arxiv.org/abs/2112.10752" title="Stable Diffusion paper">paper</a></figcaption>
</figure>
<ul>
<li><strong>Better mode coverage than GANs</strong>: You can measure the mode coverage of a generative model using precision/recall as shown in this cool paper <a href="https://arxiv.org/abs/1904.06991">"Improved Precision and Recall Metric for Assessing Generative Models"</a></li>
<li><strong>Compress by 4-8:</strong> Compressing the input image dimension by a factor <span class="arithmatex">\(4\)</span> to <span class="arithmatex">\(8\)</span> gives the greatest FID and Inception Score metrics.</li>
<li><strong>Higher throughput</strong>: Compressing the input saves a lot of computing, as shown here:</li>
</ul>
<figure>
<p><img alt="efficiency" src="../../../../images/2/efficiency.png" width="700" />
  </p>
<figcaption>The higher the compression factor, the faster the compute, and thus the higher the throughput. Different markers indicate {10, 20, 50, 100, 200} sampling steps using DDIM (more on that in another post), from right to left along each line. Benchmark datasets: CelebA-HQ (left) and ImageNet (right).</figcaption>
</figure>
<hr />
<h2 id="sdxl">SDXL</h2>
<p>What happens if the team that created Stable Diffusion has more resources at its disposal to train a bigger, more powerful model? You guessed it: <strong>Stable Diffusion XL</strong>!</p>
<p>Published in July 2023, the official release paper <a href="https://arxiv.org/abs/2307.01952">"SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis"</a> contains many great ideas. To my mind, the 3 main improvements are:</p>
<ul>
<li><strong>Scale up the model size</strong> with a  <span class="arithmatex">\(3 \times\)</span> bigger UNet and a second text encoder</li>
<li><strong>Micro-conditioning</strong>: Novel conditioning schemes and multiple aspect ratios training</li>
<li><strong>Refinement model</strong>: Use a second network to refine the samples</li>
</ul>
<figure>
<p><img alt="" src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/aa748fc2-710a-43fa-aa72-2abcd6cbd9f9/sdxl.jpg" />
  </p>
<figcaption>Samples generated by SDXL</figcaption>
</figure>
<h3 id="bigger-models">Bigger models</h3>
<p>The <a href="https://arxiv.org/abs/2102.09672">"Improved DDPMs"</a> model showed further <strong>scaling up the model size</strong> would give promising results. SDXL's authors thus use a much bigger 2.6B parameters UNet model. For comparison purposes, the latest version of Stable Diffusion was "only" 860M. They also rework the way the cross-attention blocks are placed within the model, putting <strong>more blocks in the deeper layers</strong>.</p>
<figure>
<p><img alt="" src="../../../../images/2/compare_sd_sdxl.png" width="500" />
  </p>
<figcaption>SDXL is 3 times bigger than the latest releases of SD. The text encoding part is especially bigger bc of the larger context dimension.</figcaption>
</figure>
<p>In parallel, they used not one but <strong>2 text encoding models</strong>: OpenCLIP ViT-bigG in combination with CLIP ViT-L. They essentially encoded the text prompt with both then concatenated the embeddings into one. This choice is motivated by the lack of expressiveness of the smaller encoding models, which became a bottleneck, as shown by the ever-increasing context dimension.</p>
<p><span class="arithmatex">\(\rightarrow\)</span> That's why the latest image generation models tend to have bigger and bigger encoding sub-models.</p>
<h3 id="micro-conditioning">Micro-conditioning</h3>
<ul>
<li><strong>Condition on the image size</strong>: because of the image compression step, SD needed its training images to be big enough. To avoid throwing away almost 30% of the training dataset, SD upsampled all images to a minimum size, but the model tend to blur high-resolution images at inference time because it learned the blurry aspect of the small-resolution images ... The SDXL authors showed that you can achieve higher performance by conditioning the model on the image size. Therefore, the model can train on various image resolutions, without learning the blurry effect of the lower-resolution training images. In the end, at inference time, the user can set the "apparent resolution" of the outputs.</li>
</ul>
<figure>
<p><img alt="" src="../../../../images/2/size_conditioning.jpg" width="500" />
  </p>
<figcaption>The user can set the apparent resolution through size conditioning. From <a href="https://arxiv.org/pdf/2307.01952.pdf">SDXL</a></figcaption>
</figure>
<ul>
<li><strong>Condition on the image crop</strong>: as the training implies data augmentation including cropping, SD tended to generate cropped objects. To mitigate this issue, during training, SDXL is conditioned on the cropping applied to the image. At inference, this conditioning is set to <span class="arithmatex">\((0, 0)\)</span> by default, as we usually want the generated object to be centered.</li>
</ul>
<figure>
<p><img alt="" src="../../../../images/2/sdxl_cropping.jpg" width="500" />
  </p>
<figcaption>The cropping conditioning helps reducing the risks of cropped samples. From <a href="https://arxiv.org/pdf/2307.01952.pdf">SDXL</a></figcaption>
</figure>
<h3 id="refiner">Refiner</h3>
<p>The core idea behind the refinement model of SDXL is to enhance the quality of generated images by <strong>training a secondary DDPM</strong> specifically for refinement tasks. This secondary model is trained <strong>on the latest denoising steps</strong>, allowing it to specialize in polishing and enhancing the details of images generated by the primary model.</p>
<figure>
<p><img alt="" src="../../../../images/2/sdxl_architecture.jpg" width="700" />
  </p>
<figcaption>The biggest enhancement of SDXL is the addition of a refinement module</figcaption>
</figure>
<p>Imagine <strong>Michelangelo</strong> painting a massive masterpiece, capturing the essence and overall structure of the artwork. Once the main composition is completed, he entrusts a friend <strong>Leonardo</strong> Da Vinci, proficient in intricate detailing, to refine and enhance specific elements of the painting.</p>
<p>Similarly, in SDXL, a <strong>base DDPM</strong> generates high-quality images, while the <strong>refinement model</strong> specializes in adding finer details and improving overall visual coherence. This approach ensures that the final output exhibits both the broad strokes and the subtle nuances necessary for a compelling and realistic image.</p>
<p>For instance, the picture right underneath shows how the output is improved and refined by the second model by using it for the 200 last denoising steps instead of the base UNet.</p>
<figure>
<p><img alt="" src="../../../../images/2/refinement.jpg" width="500" />
  </p>
<figcaption>Samples from SDXL without (left) and with (right) the refinement model.</figcaption>
</figure>
<p>However, the refinement model introduces certain trade-offs, notably in terms of computational overhead. By employing two UNets, the system requires more VRAM, potentially increasing computational costs and resource requirements. Despite this drawback, the refinement model offers a tangible improvement in image quality.</p>
<h3 id="additional-ideas">Additional ideas</h3>
<p>Here are some other ideas implemented in SDXL I have no time to cover:</p>
<ul>
<li><strong>Improved the autoencoder</strong>: used a bigger batch size (256 vs 9) and used exponential moving average for the weights <span class="arithmatex">\(\rightarrow\)</span> Better reconstructions</li>
<li><strong>Multi-aspect training</strong>: most LDM are trained on square images, while most real-world images are not <span class="arithmatex">\(\rightarrow\)</span> They propose a technique to finetune the model on images of variable shapes</li>
<li><strong>Heterogeneous Distribution of Transformer Blocks</strong>: Diverging from the conventional uniform distribution of transformer blocks seen in earlier models ([1,1,1,1]), SDXL employs a heterogeneous distribution ([0,2,4]). According to the authors, this "shifts the bulk of the transformer computation to lower-level features in the UNet".</li>
</ul>
<h3 id="results-and-limitations">Results and limitations</h3>
<ul>
<li>Users have a strong preference for SDXL+refiner outputs</li>
<li>SDXL still encounters difficulties when rendering long, legible text</li>
<li>Same for 3D objects with complex poses like hands</li>
</ul>
<p><span class="arithmatex">\(\rightarrow\)</span> The authors suppose that bigger text encoders, larger models and datasets will help further improve these models.</p>
<figure>
<p><img alt="" src="../../../../images/2/preferences.png" width="450" /></p>
</figure>
<h3 id="code-example">Code example</h3>
<p>Stable Diffusion XL has risen as the go-to architecture for <a href="https://huggingface.co/models?pipeline_tag=text-to-image">text-to-image generation</a> as you can see on HuggingFace. If you feel like experimenting a bit, HuggingFace hosts plenty of amazing fine-tuned models, released by the open-source community. For instance here is some code to run your own SDXL (from the <a href="https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0">HuggingFace page of SDXL</a>).</p>
<p>You first select a base model and a refiner model:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">diffusers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DiffusionPipeline</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># load both base &amp; refiner</span>
<span class="n">base</span> <span class="o">=</span> <span class="n">DiffusionPipeline</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">variant</span><span class="o">=</span><span class="s2">&quot;fp16&quot;</span><span class="p">,</span> <span class="n">use_safetensors</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="n">base</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">refiner</span> <span class="o">=</span> <span class="n">DiffusionPipeline</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;stabilityai/stable-diffusion-xl-refiner-1.0&quot;</span><span class="p">,</span>
    <span class="n">text_encoder_2</span><span class="o">=</span><span class="n">base</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="p">,</span>
    <span class="n">vae</span><span class="o">=</span><span class="n">base</span><span class="o">.</span><span class="n">vae</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
    <span class="n">use_safetensors</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">variant</span><span class="o">=</span><span class="s2">&quot;fp16&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">refiner</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
</code></pre></div>
<p>You then define how many steps you want, and what proportion should be run on the refiner:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Define how many steps and what % of steps to be run on each experts (80/20) here</span>
<span class="n">n_steps</span> <span class="o">=</span> <span class="mi">40</span>
<span class="n">high_noise_frac</span> <span class="o">=</span> <span class="mf">0.8</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;A majestic lion jumping from a big stone at night&quot;</span>
</code></pre></div>
<p>Finally, run the predictions, and give the latent output of the base model to the refiner:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># run both experts</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">base</span><span class="p">(</span>
    <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
    <span class="n">num_inference_steps</span><span class="o">=</span><span class="n">n_steps</span><span class="p">,</span>
    <span class="n">denoising_end</span><span class="o">=</span><span class="n">high_noise_frac</span><span class="p">,</span>
    <span class="n">output_type</span><span class="o">=</span><span class="s2">&quot;latent&quot;</span><span class="p">,</span>
<span class="p">)</span><span class="o">.</span><span class="n">images</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">refiner</span><span class="p">(</span>
    <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
    <span class="n">num_inference_steps</span><span class="o">=</span><span class="n">n_steps</span><span class="p">,</span>
    <span class="n">denoising_start</span><span class="o">=</span><span class="n">high_noise_frac</span><span class="p">,</span>
    <span class="n">image</span><span class="o">=</span><span class="n">image</span><span class="p">,</span>
<span class="p">)</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div>
<h2 id="conclusion">Conclusion</h2>
<p>I hope you enjoyed this reading. In a near future, I plan to share some techniques to further speed up the generation process. So stay connected, and see you in the next one! ðŸ‘‹</p>







  
  




  



      
    </article>
  </div>

          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/LouisStefanuto" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.linkedin.com/in/louis-stefanuto/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../../../..", "features": ["navigation.tabs", "navigation.expand"], "search": "../../../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../../../assets/javascripts/bundle.92b07e13.min.js"></script>
      
        <script src="../../../../../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>