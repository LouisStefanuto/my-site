
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Personal website of Louis Stefanuto">
      
      
      
      
        <link rel="prev" href="../../13/overfit2-from-ddpms-to-stable-diffusion-xl/">
      
      
        <link rel="next" href="../../../03/17/overfit4-all-the-techniques-to-control-your-text2img-models-controlnets-t2i-adapters-dreambooth-/">
      
      
      <link rel="icon" href="../../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.18">
    
    
      
        <title>Overfit#3: How does Video generation work? - Insights into Stable Video Diffusion - Louis Stefanuto</title>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/stylesheets/main.7e37652d.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#overfit3-how-does-video-generation-work-insights-into-stable-video-diffusion" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../../.." title="Louis Stefanuto" class="md-header__button md-logo" aria-label="Louis Stefanuto" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Louis Stefanuto
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Overfit#3: How does Video generation work? - Insights into Stable Video Diffusion
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
    
      <div class="md-header__source">
        <a href="https://github.com/LouisStefanuto" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    Github
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../about/" class="md-tabs__link">
        
  
  
    
  
  About

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../../../" class="md-tabs__link">
          
  
  
  Posts

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../../.." title="Louis Stefanuto" class="md-nav__button md-logo" aria-label="Louis Stefanuto" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Louis Stefanuto
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/LouisStefanuto" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    Github
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../about/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    About
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Posts
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Posts
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      <a href="../../../../" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Index
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Post series
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            Post series
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/alphafold/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AlphaFold
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/chemistry/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Chemistry
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/computer-vision/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Computer Vision
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/diffusion-models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Diffusion models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/drug-discovery/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Drug Discovery
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/embedding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Embedding
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/llm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LLM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/reinforcement-learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Reinforcement Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/robotics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Robotics
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/ssl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SSL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/video/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Video
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#motivation" class="md-nav__link">
    <span class="md-ellipsis">
      Motivation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#latent-video-diffusion-models" class="md-nav__link">
    <span class="md-ellipsis">
      Latent Video Diffusion Models
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Latent Video Diffusion Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#temporal-layers" class="md-nav__link">
    <span class="md-ellipsis">
      Temporal layers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mask-conditioning" class="md-nav__link">
    <span class="md-ellipsis">
      Mask conditioning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#temporal-interpolation" class="md-nav__link">
    <span class="md-ellipsis">
      Temporal Interpolation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#data-curation" class="md-nav__link">
    <span class="md-ellipsis">
      Data curation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-is-next" class="md-nav__link">
    <span class="md-ellipsis">
      What is next?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ethics-and-societal-impact" class="md-nav__link">
    <span class="md-ellipsis">
      Ethics and societal impact
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      References
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
  <div class="md-content md-content--post" data-md-component="content">
    <div class="md-sidebar md-sidebar--post" data-md-component="sidebar" data-md-type="navigation">
      <div class="md-sidebar__scrollwrap">
        <div class="md-sidebar__inner md-post">
          <nav class="md-nav md-nav--primary">
            <div class="md-post__back">
              <div class="md-nav__title md-nav__container">
                <a href="../../../../" class="md-nav__link">
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
                  <span class="md-ellipsis">
                    Back to index
                  </span>
                </a>
              </div>
            </div>
            
              <div class="md-post__authors md-typeset">
                
                  <div class="md-profile md-post__profile">
                    <span class="md-author md-author--long">
                      <img src="https://github.com/LouisStefanuto.png" alt="Louis Stefanuto">
                    </span>
                    <span class="md-profile__description">
                      <strong>
                        
                          Louis Stefanuto
                        
                      </strong>
                      <br>
                      Creator
                    </span>
                  </div>
                
              </div>
            
            <ul class="md-post__meta md-nav__list">
              <li class="md-nav__item md-nav__item--section">
                <div class="md-post__title">
                  <span class="md-ellipsis">
                    Metadata
                  </span>
                </div>
                <nav class="md-nav">
                  <ul class="md-nav__list">
                    <li class="md-nav__item">
                      <div class="md-nav__link">
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 19H5V8h14m-3-7v2H8V1H6v2H5c-1.11 0-2 .89-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2h-1V1m-1 11h-5v5h5z"/></svg>
                        <time datetime="2024-02-24 00:00:00+00:00" class="md-ellipsis">February 24, 2024</time>
                      </div>
                    </li>
                    
                    
                      <li class="md-nav__item">
                        <div class="md-nav__link">
                          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9 3v15h3V3zm3 2 4 13 3-1-4-13zM5 5v13h3V5zM3 19v2h18v-2z"/></svg>
                          <span class="md-ellipsis">
                            in
                            
                              <a href="../../../../category/diffusion-models/">Diffusion models</a></span>
                        </div>
                      </li>
                    
                    
                      
                      <li class="md-nav__item">
                        <div class="md-nav__link">
                          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 20a8 8 0 0 0 8-8 8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8m0-18a10 10 0 0 1 10 10 10 10 0 0 1-10 10C6.47 22 2 17.5 2 12A10 10 0 0 1 12 2m.5 5v5.25l4.5 2.67-.75 1.23L11 13V7z"/></svg>
                          <span class="md-ellipsis">
                            
                              11 min read
                            
                          </span>
                        </div>
                      </li>
                    
                  </ul>
                </nav>
              </li>
            </ul>
            
          </nav>
          
        </div>
      </div>
    </div>
    <article class="md-content__inner md-typeset">
      
        
  



  
  


<h1 id="overfit3-how-does-video-generation-work-insights-into-stable-video-diffusion"><strong>Overfit#3:</strong> How does Video generation work? - Insights into Stable Video Diffusion</h1>
<p><img alt="" src="../../../../images/3/main.jpg" /></p>
<!-- more -->

<h2 id="motivation">Motivation</h2>
<p>Guys, generating videos is incredibly challenging ... I mean, truly, immensely challenging.</p>
<p>For the past two years, we've been treated to a constant stream of cutting-edge text-to-image models on Hugging Face: DALLE, Stable Diffusion, SDXL, SDXL Turbo... The text-to-image community is positively buzzing, much to our delight.</p>
<p>Meanwhile, on Hugging Face, the video generation space is eerily quiet. Too quiet. The reason? If generating images was complex, <strong>generating videos is hell on earth</strong>.</p>
<div class="admonition quote">
<p class="admonition-title">In theory, generating a video isn't much harder than generating a set of images, is it?</p>
</div>
<p>Well, in practice, yes. A video comprises a sequence of images, displayed at 30 frames per second. But, you also have to address some really tricky challenges:</p>
<ul>
<li><strong>How to enforce a high consistency between frames?</strong> Text-to-image models lack temporal understanding, so generating frames that logically follow one another is crucial.</li>
<li><strong>How to reduce the insane computational cost?</strong> Producing just one second of video equates to generating 30 frames. For a minute-long video, that's a staggering 900 frames... I hope your credit card is within reach. Your bank adviser won't appreciate what is gonna happen.</li>
<li><strong>What dataset to use?</strong> Unlike text or images, there are far fewer high-quality open datasets.</li>
</ul>
<figure>
<p><img alt="Stable Video Diffusion" src="https://media1.tenor.com/m/rhzC9_tMUukAAAAd/will-smith-eating-spaghetti-will-smith.gif" width="350" />
  </p>
<figcaption>"Will Smith eating spaghettis". SOTA a few months ago.<br>See how hard it is to make realistic coherent videos.</figcaption>
</figure>
<div class="admonition quote">
<p class="admonition-title">What does research suggest about these challenges?</p>
</div>
<p>Unfortunately, as of early 2024, many video-generation models remain proprietary, such as Pika, Gen2, and SORA. Consequently, insights into the training techniques of top companies are limited. Fortunately, the Stability AI team continues to share their techniques through open-sourcing their innovations. In this discussion, we will delve into the fundamental technologies that underpin video generation, with a focus on <a href="https://stability.ai/news/stable-video-diffusion-open-ai-video-model"><strong>Stable Video Diffusion</strong></a> (SVD), which stands out as one of the leading open-source models in this field in early 2024.</p>
<div class="admonition note">
<p class="admonition-title">Reference papers</p>
<ul>
<li><a href="https://arxiv.org/abs/2304.08818">Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models (apr 2023)</a></li>
<li><a href="https://arxiv.org/abs/2311.15127">Scaling Latent Video Diffusion Models to Large Datasets (nov 2023)</a> <span class="arithmatex">\(\rightarrow\)</span> Stable Video Diffusion release</li>
</ul>
</div>
<figure>
<p><img alt="Stable Video Diffusion" src="https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt/resolve/main/output_tile.gif" />
  </p>
<figcaption>Stable Video Diffusion generate realistic videos. But their duration is still limited to just a few seconds.</figcaption>
</figure>
<hr />
<h2 id="latent-video-diffusion-models">Latent Video Diffusion Models</h2>
<p>To train an video generation model, most approaches converged to a <strong>three-step</strong> training process:</p>
<ol>
<li><strong>Text-to-image pretraining</strong>: Pretrain a text-to-image model (Stable Diffusion 2.1 for ex.) to generate images. This step is optional, but research indicates that it yields significantly improved results. This is logical, as leveraging the knowledge from the image dataset enhances performance.</li>
<li><strong>Text-to-video pretraining</strong>: Add intermediate temporal layers in the pretrained text-to-image model, and train it as before but on a large video dataset, to align images in a temporally consistent manner.</li>
<li><strong>Text-to-video finetuning</strong>: Same as (2) but use a higher resolution dataset, to finetune the super resolution modules.</li>
</ol>
<div class="admonition note">
<p class="admonition-title">For your information ...</p>
<p>I already covered step 1 in two posts about how to train a text-to-image model: <a href="../../01/overfit1-denoising-diffusion-probabilistic-models---basics-of-diffusion-models/"><strong>#1-DDPM</strong></a> and <a href="../../13/overfit2-from-ddpms-to-stable-diffusion-xl/"><strong>#2-SDXL</strong></a>. In the following sections, <strong>I will focus on steps 2 and 3</strong>: how to turn a text-to-image pretrained model into an image-to-video model.</p>
</div>
<h3 id="temporal-layers">Temporal layers</h3>
<p>An LDM - Latent Diffusion Model - is able to synthetize high-quality, realistic, independent, images. Yet, it is unable to enforce strong consistency between frames.</p>
<blockquote>
<p>For instance, an LDM can generate a batch of 32 "An astronaut riding a horse" images, but, as each image is generated independently from the other, they won't be consistent.</p>
</blockquote>
<p>That is because LDM are <strong>spatial</strong> models. They have no <strong>temporal</strong> understanding. They can place pixels next to each other in the <span class="arithmatex">\(XYZ\)</span> dimensions, but not in the time <span class="arithmatex">\(T\)</span> dimension. Their layers are thus called <strong>spatial layers</strong>.</p>
<p>The key idea to turn LDMs into Video Generators is therefore to add <strong>temporal layers</strong>, to enforce consistency between the frames. We call this step <strong>temporal alignment</strong>, hence the name of the article <a href="https://arxiv.org/abs/2304.08818">"Align your Latents" (2023)</a>.</p>
<p>There are plenty of ways to insert the temporal layers, with a common solution being to interleave temporal blocks between spatial blocks.</p>
<figure>
<p><img alt="temporal" src="../../../../images/3/interleaved_blocks.jpg" width="500" />
  </p>
<figcaption>Temporal blocks are interleaved between the pretrained spatial blocks. Image based on the Stable Diffusion architecture.</figcaption>
</figure>
<div class="admonition quote">
<p class="admonition-title">OK. So we have two sets of blocks, temporal blocks and spatial blocks. What sets them apart?</p>
</div>
<p>A simple way to depict the distinct traits of each block is by plotting their <strong>context windows</strong> in the space-time domain, which are directly correlated to their role. In the diagram below, each row represents an individual frame. Spatial operations are illustrated horizontally, while columns illustrate how pixel values evolve over time. Consequently, a vertical attention operation signifies a purely temporal mechanism, effectively aligning the latents across time. The temporal Conv3D block is an in-between block, both spatial and temporal, but more local, as indicated by its presence its short range context window in both dimensions.</p>
<figure>
<p><img alt="temporal" src="../../../../images/3/temporal_techniques.jpg" />
  </p>
<figcaption>Modified from J. Bengochea [1], from original paper [2]</figcaption>
</figure>
<p>These blocks are not the only choices, as one could use full-context blocks that attend to all pixels across all frames (full spatio-temporal attention) or solely to past frames (causal spatio-temporal attention). However, their computational expense is inherently much higher.</p>
<center>

| Block | Role | This layer ensures | Input |
| -------------- | ------- | ------------------------------ | - |
| **Spatial** | Attends to only one image at a time: these are the blocks from the pretrained LDM (step 1) | Spatial consistency | A batch of 3D independent frames |
| **Temporal Conv3D** | Aligns pixels with their closest neighbors in the space-time space | Short-term space-time coherence | A batch of 4D videos, i.e. a batch of frames grouped by video |
| **Temporal Attention** | Aligns a single pixel with itself through time |  Long-term temporal coherence | The sequence of values of the pixel during the whole video, i.e. a sequence of $T$ tokens, each token is a 1D-vector of the channels at a given frame |

</center>

<p>In <a href="https://arxiv.org/abs/2304.08818">"Align your Latents" (2023)</a>, the authors use the three types of blocks, to "spread consistency" across spatial and temporal dimensions.</p>
<figure>
<p><img alt="temporal" src="../../../../images/3/temporal_layer.jpg" />
  </p>
<figcaption>From "Align your latents"</figcaption>
</figure>
<div class="admonition success">
<p class="admonition-title">Main takeaway</p>
<p>To my opinion, all you need to remember is that Conv3D temporal blocks enforce local space-time coherence, while attention-based temporal blocks ensure long-term temporal consistency.</p>
</div>
<div class="admonition quote">
<p class="admonition-title">Questions you may ask</p>
<ul>
<li>
<p><strong>Should I train the whole network or only the temporal layers?"</strong></p>
<p>No consensus on that. <a href="https://arxiv.org/abs/2304.08818">Align your Latents</a> freezes the spatial layers and train the temporal layers on video data only. <a href="https://arxiv.org/abs/2311.15127">Stable Video Diffusion</a> trains both spatial and temporal layers.</p>
</li>
<li>
<p><strong>Should I train the encoder/decoder parts of the Latent Diffusion Model?</strong></p>
<p>Yes, but only the decoder. In <a href="https://arxiv.org/abs/2304.08818">Align your Latents</a>, the authors stress out that adding temporal layers to the decoder is key. Indeed, "the autoencoder of the LDM is trained on images only, causing flickering artifacts when encoding and decoding a temporally coherent sequence of images." It makes sense as even if the latent representations of all frames are close in the embedding space, the decoder may decode them slightly differently, causing artifacts.</p>
</li>
<li>
<p><strong>I notice there are many reshape operations. Why is the batch reshaped so many times?</strong></p>
<p>Each sub-block expects a different input. Either independent frames, or videos, or pixel values sequences. Hopefully, as all training videos have the same number of frames <span class="arithmatex">\(T\)</span> and the same frame dimensions (<span class="arithmatex">\(H \times W \times C\)</span>), switching from a format to another is a simple <code>np.reshape</code> (or equivalent) operation.</p>
<p><figure markdown>
<img alt="different layers" src="../../../../images/3/different_layers.png" />
<figcaption>Each layer type expects a specific format. The channel dimension is not represented.</figcaption>
</figure></p>
</li>
</ul>
</div>
<h3 id="mask-conditioning">Mask conditioning</h3>
<p>Temporal layers allow us to transform our pretrained text-to-image model into an image-to-video model capable of producing short videos, made of a few frames. However,  <a href="https://arxiv.org/abs/2304.08818">"Align your Latents" (2023)</a> has demonstrated that <strong>this approach faces limitations when attempting to generate longer videos</strong> with a greater number of frames.</p>
<p>One method to overcome this limitation is to train the model to generate frames conditioned on the preceding frames of the sequence. This approach involves generating an initial short video and then generating subsequent videos using the last frames of the preceding video sequence. This iterative process can be repeated to extend the length of the generated video content.</p>
<p>In "Align your Latents" (2023), the authors employ a conditioning technique that provides the network with a sequence of frames, where the majority are masked, except for 0, 1, or 2 frames. This approach draws inspiration from the paper <a href="https://arxiv.org/abs/2111.06377">"Masked Autoencoders Are Scalable Vision Learners" (2021)</a> by Facebook, which enhances autoencoders through the use of masking techniques.</p>
<p><span class="arithmatex">\(\Rightarrow\)</span> Using this technique, the model is both a <strong>denoising</strong> model (it converts noise into frames) and a <strong>prediction</strong> model (it predicts the missing frames, based on the not noised ones).</p>
<figure>
<p><img alt="conditioning" src="../../../../images/3/conditioning.jpg" />
  </p>
<figcaption>The temporal block is conditioned with 0, 1 or 2 context frames.</figcaption>
</figure>
<figure>
<p><img alt="masking" src="https://miro.medium.com/v2/resize:fit:1120/1*l8zPV1sSDmEPbwHTDh5Rzw.png" width="500" />
  </p>
<figcaption>The fundamental concept behind "Masked Autoencoders Are Scalable Vision Learners" is to train the autoencoder to reconstruct an image it has only partially observed.</figcaption>
</figure>
<h3 id="temporal-interpolation">Temporal Interpolation</h3>
<p>Okay, we have a model <strong>capable of generating dozens of frames</strong>, which is a significant advancement. However, this may still not suffice, especially for achieving high frame rates. So, what's the simplest way to create a credible data point from two existing data points? The answer: <strong>interpolation</strong>.</p>
<p>A common strategy adopted by image-to-video models is to first generate some key frames. Using the mask conditioning method discussed earlier, the model is fed with two frames as conditioning, and then tasked with inferring three intermediate frames. Through this approach, we enhance the frame rate by a factor of 4.</p>
<p>This process can be iterated. For example, in "Align your Latents", the authors employ a double interpolation technique,  resulting in a <span class="arithmatex">\(\times 16\)</span> increase in frame rate, all within a single network.</p>
<blockquote>
<p><em>Additional notes: To complete the process, the decoder then translates all latent frames back into the pixel space, after which they undergo upsampling through a Super Resolution (SR) model. This cascaded approach significantly diminishes the computational burden of models while preserving their high-resolution capabilities.</em></p>
</blockquote>
<div class="admonition success">
<p class="admonition-title">Putting everything together</p>
<p>You now deeply understand the inference process of SVD ðŸ”¥</p>
<ul>
<li>(1) First a few frames are generated in the latent space. </li>
<li>(2) and (3) The model then interpolates twice to increase the frame rate. </li>
<li>(4) Finally the decoder decodes the latents into pixel frames. </li>
<li>(5) An additional upsampling model is applied to polish the results.</li>
</ul>
<p><figure markdown>
<img alt="inference" src="https://research.nvidia.com/labs/toronto-ai/VideoLDM/assets/figures/video_ldm_stack.png" width="500" />
</figure></p>
</div>
<hr />
<h2 id="data-curation">Data curation</h2>
<p>To my opinion, <a href="https://arxiv.org/abs/2311.15127">Stable Video Diffusion</a>'s success <strong>lies in its dataset</strong>.</p>
<p>Indeed, <a href="https://arxiv.org/abs/2311.15127">A. Blattmann &amp; al.</a> showed that <strong>not all videos are suitable candidates for training</strong>. Too short, frequent cut edits, fading in and out, text ... Most videos you could scrap on the web are unsuitable. Additionally, many videos are just static scenes, making them poor candidates to learn motion. Yet, the authors demonstrated how an extensive <strong>curation strategy</strong> can enhance model performance.</p>
<figure>
<p><img alt="" src="https://stablevideodiffusion.cc/src/picture/811.gif" width="450" />
  </p>
<figcaption>Videos generated by Stable Video Diffusion.</figcaption>
</figure>
<p>To build their dataset, SVD's authors began with <a href="https://www.v7labs.com/open-datasets/the-webvid-10m-dataset">WebVid-10M</a>, a standard video dataset. Through preprocessing techniques and automatic video annotation (using LLMs), they expanded this dataset to an impressive ~500M videos, equivalent to a staggering <strong>212 years</strong>' worth of content.</p>
<p>Then, they rigorously filtered this huge dataset. Their filtering process involved several key steps, including:</p>
<ul>
<li><strong>Reducing the number of cuts</strong> per video by segmenting the videos into smaller sub-videos.</li>
<li><strong>Identifying and removing static videos</strong> â€” an easily recognizable characteristic indicated by low optical flow (the mean pixel difference between frames).</li>
<li><strong>Eliminating videos with excessive textual content</strong> using optical character recognition (OCR) techniques.</li>
</ul>
<blockquote>
<p><span class="arithmatex">\(\rightarrow\)</span> The extent of the filtering process varied depending on the level of selectivity applied. Consequently, their final datasets ranged from containing 152M to 2.3M videos, 300 frames/video on average i.e ~10sec/video.</p>
</blockquote>
<div class="admonition success">
<p class="admonition-title">Let's keep in mind: garbage in, garbage out.</p>
</div>
<hr />
<h2 id="what-is-next">What is next?</h2>
<p>The video generation ecosystem is still in its infancy. However, there is no doubt that all major players in the field (including tech giants and research labs) are actively working to crack the case. After LLMs and image generators, the next step is video generation. For me, there are still two main challenges that prevent us from achieving truly high-performing models:</p>
<ol>
<li><strong>Better Datasets</strong>: Improving curation techniques is crucial for refining and expanding video datasets. While there are numerous open-source image datasets available, there is also a growing need for more open video datasets. Expectations are high for the community to release more comprehensive and diverse video datasets in the coming months.</li>
<li><strong>Enhanced Spatial-Temporal Architectures</strong>: Ensuring temporal consistency remains a significant challenge. While the architectures developed thus far represent a promising initial response, <strong>they are far from perfect</strong>. Full-transformer architectures might hold the key to overcoming this challenge (SORA ðŸ‘€) ... Only time will reveal if this approach will emerge as the definitive solution for addressing temporal consistency.</li>
</ol>
<figure>
<p><img alt="SORA" src="https://miro.medium.com/v2/resize:fit:1400/1*20Z1I1Ze5Z1NdgiLbEGqYA.gif" width="450" />
  </p>
<figcaption>SORA, the OpenAI banger we are all waiting for.</figcaption>
</figure>
<p>Finally, video generation models open an extremely promising door to 3D generation.</p>
<p>Indeed, a notable feature of video generators is their ability to create videos showing a camera rotating around an object <strong>using only an image of that object</strong>. While this may not initially seem remarkable, its practical significance becomes evident when combined with a 3D reconstruction tool. This combination presents an opportunity to generate 3D models directly from a single image.</p>
<figure>
<p><img alt="plage" src="../../../../images/3/plage.jpg" width="450" />
  </p>
<figcaption>Here is an example I recently came across, taken from a video shared as part of the SORA release. From <a href="https://twitter.com/tombielecki/status/1760353822399242410"> Twitter </a>.</figcaption>
</figure>
<hr />
<h2 id="ethics-and-societal-impact">Ethics and societal impact</h2>
<p>The pace of progress in the field of video generation is remarkable.</p>
<p>However, as AI practitioners, we must remain acutely aware of the societal implications of such advancements. Up until now, these models have often lacked certain elements that would easily reveal the falsity of generated videos. However, the recent unveiling of SORA by OpenAI indicates that technology is advancing at a rate beyond our expectations.</p>
<p>The level of intricacy demonstrated by SORA is both captivating and, let's be honest, somewhat concerning. The potential for misuse of video models surpasses that of image generation models. When combined with video editing tools, these new technologies have the capability to revolutionize business applications while also serving as potent tools for disinformation.</p>
<div class="admonition note">
<p class="admonition-title">That's all for today! If you liked this post, share and comment ðŸŒŸ</p>
</div>
<hr />
<h2 id="references">References</h2>
<p>[1] J. Bengochea, Marvik blog, <a href="https://blog.marvik.ai/2024/01/30/diffusion-models-for-video-generation/">"Diffusion models for video generation"</a>, 2024.</p>
<p>[2] Anonymous authors, paper under review as submission to TMLR, <a href="https://openreview.net/pdf?id=sgDFqNTdaN">"Video Diffusion Models - A Survey"</a>.</p>
<p>[3] K. He &amp; al., Facebook, <a href="https://arxiv.org/abs/2111.06377">"Masked Autoencoders Are Scalable Vision Learners"</a>, 2021.</p>
<p>[4] A. Blattmann &amp; al., <a href="https://arxiv.org/abs/2304.08818">"Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models"</a>, apr 2023.</p>
<p>[5] A. Blattmann &amp; al., Stable Video Diffusion release paper, <a href="https://arxiv.org/abs/2311.15127">"Scaling Latent Video Diffusion Models to Large Datasets"</a>, nov 2023.</p>







  
  




  



      
    </article>
  </div>

          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/LouisStefanuto" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.linkedin.com/in/louis-stefanuto/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../../../..", "features": ["navigation.tabs", "navigation.expand"], "search": "../../../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../../../assets/javascripts/bundle.92b07e13.min.js"></script>
      
        <script src="../../../../../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>